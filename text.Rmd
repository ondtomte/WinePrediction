---
title: "Project"
author: "Simon Almerstr?m Przybyl & Daniel ?hman"
date: "July 27th 2015"
output: html_document
---

#1. INTRO / EXPLAIN DATA SET AND TASK

For our project, we have chosen to examine a wine data set. The data set contains measures of chemical levels in wine as its input variables, and the quality of wine as its target variable. The measured chemical levels ranged from the pH level of the wine to the alcohol percentage of the wine, in total 11 different chemical levels were measured on a continuous linear scale. The quality of the wine was measured on a discrete scale from 0 to 10, though the actual observed range was only from 3 to 9. It should also be noted that the quality of the wine was graded by letting individuals taste the wine, the quality scale therefore reflects expert opinion.

```{r}
#download.file(url="https://archive.ics.uci.edu/ml/machine-learning-databases/wine-qual#ity/winequality-white.csv",destfile = "winequality-white.csv")
wine<-read.csv("winequality-white.csv",sep=";")
str(wine)
```

Our task is thus to investigate how different chemical levels affect the quality of the wine, and we split this task into two subtasks:

1 - We view the quality of wine as a continuous variable allowed to take any value on the interval [0,10], this view of the problem yields a regression problem.

2 - We define a good wine to be a wine having quality larger than or equal to 6, and a poor wine to be a wine having quality strictly less than 6. This yields a binary classification problem of separating good wine from poor wine.

Regarding the accuracy of the different subtasks:

In the regression subtask we are of course seeking to minimize the MSE as far as possible. Moreover, we definitely want our regression model to give a lower MSE than the MSE we obtain in our baseline model, this model consists of simply guessing the mean quality of the training.
```{r}
hist(wine$quality,col="red",main="White wine quality distribution",xlab="Quality")
```

#Note that the histogram plots the quality of the training observations, we will describe the process of #splitting the data into training, validation and test sets more thorughly later.

Since our training data is approximately normally distrubuted around 6 (with of variance of 0.7723022), our baseline model will perform quite well (this model yields a validation MSE of 0.7705384 and a test MSE of 0.7779192). Therefore, in order to justify the existence of our more sophisticated models, we want them to give a much lower MSE.

How low we want to push down the MSE for us to be satisfied with our models is a difficult question and depends on how are models are supposed to be used. We believe a resonable purpose for our models is to act as a tool for beverage stores when they decide which wine they should keep in stock. However, it is still difficult to define how low the MSE should be for us to be content: When one is for example constructing a bridge, one will probably want the prediction of the target value to be precise enough so the probability of the bridge collapsing is negligent. In our case, our regression models can not really be said to have such a clear goal: We can basically only tell if one given model is better than another given model, not if they are good enough in an absolute sense.

Given the discussion above, any quantitative measure of how much we want to reduce our test MSE is completely arbitrary. However, we pretend that the marketing department of the beverage store has demanded that the test MSE of a sufficient model should be 50% lower than the test MSE of our basline model, such a reduction in MSE would let the marketing department advertise our model as being "50% better than chance".

In the classification subtask, our main interest is to have a high precision rate, intuitively because we seek to minimize the risk of someone intending to buy a good wine but actually getting a poor wine. We believe a 90% precision rate is sufficiently high for this purpose.


#2. DATA CLEANING

We downloaded the data set from the UCI Machine Learning Repository [link]( https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/). At the repository, data regarding both white and red wine was available in two separate files, however we chose to only use the data for the white wine since we believe that the quality of wine with different colours may be affected differently by varying chemical levels. Moreover, expert opinion varies when judging white wine compared to red wine.

The data set did not contain any unknown values. Initially, we did not search for any outliers but after having constructed a few models, we noticed that our validation MSE was systematically lower than our training MSE. When this was noticed, we tried to resplit our data into training, validation and test sets a few times, however the pecularity of obtaining lower validation MSE than training MSE was present for most of the splits. The pecularity was especially pronounced for the simpler models. This lead us to believe that there were outliers present in our data: We believe that having outliers present in the training data will not affect the overall behavior of simpler models but will push up the training MSE highly for these models (since the prediction for the outliers will be far off).

We searched for outliers (i.e. observations deviating extremely) by examining the Euclidean norm of the attributes of the observations:

```{r}
require(scales)
wine <- cbind(as.data.frame(scale(wine[,-12],center = TRUE)),quality=wine[,12])
wine2 <- wine
euc <- sqrt(wine[,1]^2+wine[,2]^2+wine[,3]^2+wine[,4]^2+wine[,5]^2+wine[,6]^2+wine[,7]^2+wine[,8]^2+wine[,9]^2+wine[,10]^2+wine[,11]^2)
plot(sort(euc),type="l", xlab = "Sorted index", ylab = "Euclidean norm")
sorted<-sort(euc, index.return = TRUE)$ix
wine<-wine[-sorted[c(1:80,(length(sorted)-200):length(sorted))],]
euc <- sqrt(wine[,1]^2+wine[,2]^2+wine[,3]^2+wine[,4]^2+wine[,5]^2+wine[,6]^2+wine[,7]^2+wine[,8]^2+wine[,9]^2+wine[,10]^2+wine[,11]^2)
lines(y=sort(euc),x=51:(nrow(wine)+50), col = alpha("red", 0.5), lw=6)
```

The black line shows the Euclidean norm of all of the observations in the wine data set, ranging from the one with the lowest norm to the one with the highest. Since there are clearly a few observations deviating extremely in terms of their Euclidean norm, we chose to remove these observations and only keep the line segment marked in red. The following histogram shows the quality distribution of the removed observations:

```{r}
hist(wine2$quality[sorted[c(1:80, (length(sorted)-200):length(sorted))]], col = "red", main = "Removed data", xlab = "Quality")
```

The histogram shows that the removed observations did not come from any particular quality class (they are distributed similarly to the training data set).

RUBRIK

The wine data set was split into training, validation and test sets by doing a random 64-16-20 split while preserving relative ratios of different labels of the target variable (note that this split is done by first doing a 80-20 split into a combined training-validation set and test set, followed by a 80-20 split of the combined training-validation set). For the binary classification we created a new target variable which is either true or false depending on if the wine is good or poor. Note that the same data split was used for both the regression problem and the classification problem. Finally, we scaled and centered all of the attributes except for the target variable.

```{r}
GOOD_WINE=5
library(caTools)
set.seed(24)
s <- sample.split(Y = wine$quality,SplitRatio = 0.8)
test <- wine[s==FALSE,]
temp <-wine[s==TRUE,]
s <- sample.split(Y = temp$quality,SplitRatio = 0.8)
validation <- temp[s==FALSE,]
train <- temp[s==TRUE,]
```

#3 & 4. FEATURE IMPORTANCE SCORES / PCA

We used PCA to find the successive orthogonal directions of maximum variance:

```{r}
PCA <- prcomp(train[,-12], center = FALSE, scale. = FALSE)
PCA$sdev
```

Intuitively PCA defines a set of new base vectors of our data (corresponding to a rotation) and also gives the standard deviations of our data in these directions, the idea is to keep the directions of high variance since these are assumed to be the most informative (at least given a linear relation between the attrbiutes and the target variable).

Since the standard deviations decrease quite slowly without any sharp decline, except for the step from the 10th to the 11th principle component, there is not any natural cut-off point for how many principle components we should choose. We therefore choose to express our data using the first i principle components, for every i from 1 to 11, and create a new data frame for each such transformation.

We also create new data frames where we have projected our original data onto 2nd and 3rd order polynomials, note that these polynomials contain 11 variables and thus have a lot of cross terms. We do not project onto polynomials of higher order because of computational limitations, and we do not believe that such polynomials would yield better models since they would most likely overfit to the training data. When we create models, we will simultaneously create a given model (for example linear regression) for all of these data frames.

```{r, echo=FALSE}
#Create data set with diffrent attribute
trainPoly<-lapply(2:3,function(x){
  foo<- with(train,polym(fixed.acidity,volatile.acidity,citric.acid,residual.sugar,chlorides,free.sulfur.dioxide,total.sulfur.dioxide,density,pH,sulphates,alcohol,degree = x,raw=TRUE))
foo<-as.data.frame(foo)
foo$quality=train$quality
list(foo)
})

validationPoly<-lapply(2:3,function(x){
  foo<- with(validation,polym(fixed.acidity,volatile.acidity,citric.acid,residual.sugar,chlorides,free.sulfur.dioxide,total.sulfur.dioxide,density,pH,sulphates,alcohol,degree = x,raw=TRUE))
  foo<-as.data.frame(foo)
  list(foo)
})


PCA <- prcomp(train[,-12], center = FALSE, scale. = FALSE)
PCA.validation<-lapply(1:11, function(i){
    if (i == 1){
    foo <- as.matrix(validation[,-12]) %*% PCA$rotation[,1:i]
    list(data.frame(PC1 = foo))
  } else {
     list(as.data.frame((as.matrix(validation[,-12]) %*%
                                         PCA$rotation)[,1:i]))
  }
})

PCA.train<-as.data.frame(cbind(PCA$x,quality=train$quality))

dataSets<- list(normal=list(train=train,validation=validation),
                poly2=list(train=as.data.frame(trainPoly[[1]]),validation=as.data.frame(validationPoly[[1]])),
                poly3=list(train=as.data.frame(trainPoly[[2]]),validation=as.data.frame(validationPoly[[2]])),
                pca10=list(train=PCA.train[,c(1:10,12)],validation=as.data.frame(PCA.validation[[10]])),
                pca9=list(train=PCA.train[,c(1:9,12)],validation=as.data.frame(PCA.validation[[9]])),
                pca8=list(train=PCA.train[,c(1:8,12)],validation=as.data.frame(PCA.validation[[8]])),
                pca7=list(train=PCA.train[,c(1:7,12)],validation=as.data.frame(PCA.validation[[7]])),
                pca6=list(train=PCA.train[,c(1:6,12)],validation=as.data.frame(PCA.validation[[6]])),
                pca5=list(train=PCA.train[,c(1:5,12)],validation=as.data.frame(PCA.validation[[5]])),
                pca4=list(train=PCA.train[,c(1:4,12)],validation=as.data.frame(PCA.validation[[4]])),
                pca3=list(train=PCA.train[,c(1:3,12)],validation=as.data.frame(PCA.validation[[3]])),
                pca2=list(train=PCA.train[,c(1:2,12)],validation=as.data.frame(PCA.validation[[2]])),
                pca1=list(train=PCA.train[,c(1,12)],validation=as.data.frame(PCA.validation[[1]])))
                
dataSets$normal$validation$quality=NULL
```

We examine the linear dependency between the attributes and return those who are most linearly dependent (those with largest absolute value of correlation):

```{r}
#cor
cor_wine<-cor(train)
cor_wine[lower.tri(cor_wine,diag=TRUE)] <-NA
df_wine <- as.data.frame(as.table(cor_wine))
df_wine <- df_wine[order(-abs(df_wine$Freq)),]
#Top 10 highest correlated attributes
colnames(df_wine)<- c("Var1","Var2","Cor")
df_wine[1:10,]
```

If we were to remove some features, it would be reasonable to remove for example residual.sugar and total.sulfur.dioxide since density already provides information about these attributes.

KOMMENTAR.

We also examine the mutual information criterion:

```{r}
#Shared information
require(plyr)
require(reshape)
require(entropy)
my_fun<-function(x){freq <- count(train,c("quality",x))
colnames(freq) <- c("Var1","Var2","Freq")
freq2d<-cast(freq, Var1 ~ Var2,fill=0,value ="Freq")
mi.plugin(freqs2d = freq2d)}

cn<- colnames(wine[,-12])
mi <- sapply(cn,my_fun)
sort(mi,decreasing = TRUE)
```

The fact that density has the highest mutual information criterion intuitively matches our results about the linear dependency of the attributes: By using the linear dependencies, knowing the density of wine provides us with information about its residual.sugar, alcohol and total.sulfur.dioxide values. Knowing four attributes, we most likely have a higher probability of knowing the quality of the wine than if we only know one attribute.

#5. MODELS

##5.1 REGRESSION

```{r, echo=FALSE}
MSE <- function(x,y){
  mean((x - y)^2, na.rm = TRUE)}
```

We begin by simply regressing quality on each of the individual attributes. The highest mutual information was observed between quality and density, we therefore naively except this model to perform best (given that the relation between density and quality is actually linear, plotting the data shows a slight such tendency):

```{r}
lm.ind <- list()
for (i in 1:11) {
  model<-lm(as.formula(paste("quality~", names(train)[i])),train)
  pred<-predict(model,validation)
  lm.ind[[names(train)[i]]] <- list(validationMSE=MSE(pred,validation[12]),
  trainMSE=mean(model$residuals^2))}
```

The models performed badly, the only model with descent performance was the alcohol model (note that alcohol occured frequently in the correlation scores).

We continue with more advanced linear models:

```{r}
library(boot)
linear.models <- lapply(dataSets, function(x){
  model<-glm(quality~.,data=x$train)
  pred<-predict(model,as.data.frame(x$validation))
  cv<-cv.glm(as.data.frame(x$train), model, K = 5)
  list(validationMSE=MSE(pred,validation[12]),
  trainMSE=mean(model$residuals^2),
  trainCV=cv$delta,mceValidation=mean((pred<GOOD_WINE)!=(validation$quality<GOOD_WINE)),model=model)
})
```

The difference in validation MSE between the best models is quite small, but polynomial regression of degree 2 performs best. Polynomial regression of degree 3 seems to overfit (since the difference between its training and validation MSE is high compared to the other models), yet still performs second best. It can also be noted that the CV errors (raw and adjusted) lie close to the acutal validation errors, as expected. Finally, the more PCA components we use the better the model gets, suggesting that all variables are relevant (it can also be noted that using all PCA components gives the same result as performing linear regression on the original data, as expected).

We also construct a set of lasso models, these models give a penalty on the absolute value of the linear coefficients, a parameter lambda decides the weight of the penalty. These models effectively corresponds to successively removing our attributes (as $/lambda$ grows larger) and might also thus give us information regarding if some of our attributes are redundant:

```{r, cache=TRUE}
require(lars)

lasso.models <- lapply( dataSets, function(x){
  m<-as.matrix(x$train[-ncol(x$train)])
  models<-lars(x = m, y = as.numeric(x$train$quality), type = "lasso")
  
  cv<-cv.lars(x = m, y = as.numeric(x$train$quality), type = "lasso", plot.it = FALSE, K = 5)
  pred<-predict(models, as.matrix(x$validation))
  mse<-sapply(1:ncol(pred$fit),function(x){MSE(pred$fit[,x],validation$quality)})
  bestMse<-list(min=min(mse),bestLassoModel=which.min(mse))
  list(validationMSE=mse,CV=cv, model=models,bestMse=bestMse,bestMceValidation=mean((pred$fit[,bestMse$bestLassoModel]>GOOD_WINE)!=(validation$quality>GOOD_WINE)))
})
```

The lasso models corresponding to polynomial regression of degree 3 gave the lowest MSE. This can actually be said to match the results from our ordinary linear models: Lasso regression on the poly3 data approximately corresponds to successively removing terms from the fitted 3rd order polynomial. Since polynomial regression of degree 2 gave the best linear model, and polynomial regression of degree 3 gave the second best, it seems reasonable to find an even better polynomial fit if we let the degree of the polynomial "lie between" 2 and 3.

```{r}
p<-sapply(lasso.models, function(x){min(x$validationMSE)})
barplot(p,las=2,main="Best Lasso model",ylab = "Validation MSE")
```

```{r}
sum(coef(lasso.models$poly3$model)[which.min(lasso.models$poly3$validationMSE),]>0)
ncol(coef(lasso.models$poly3$model))
```

```{r}
lambda<-c(0,lasso.models$poly3$model$lambda)
lambda<-sort(lambda)
plot(x=lambda,y=lasso.models$poly3$validationMSE,type="l",xlab=expression(lambda),ylab="Validation MSE", main="Lasso Poly3")
```

For the untransformed data, the more attributes that were removed the higher the MSE became. This fact also indicates that all of the attributes are relevant.

Moreover, we also construct ridge models which penalize the absolute value of the linear coefficients:

```{r,eval=TRUE,cache=TRUE}
predict.ridge<-function(model,x,i){
  scale(x,center = model$xm,scale = model$scales)%*%model$coef[i,] +model$ym
}

library(MASS)
ridge.models<-lapply(dataSets[-13],function(x){
  models<-lm.ridge(formula = quality~.,data=x$train,lambda = seq(0,1500,by=10))
  predictionsValidation <- scale(x$validation,center = models$xm,scale = models$scales)%*%models$coef +models$ym
  validationMSE <- apply(predictionsValidation,MARGIN=2,MSE,y=validation$quality)
  
  predictionsTrain<-  scale(x$train[-ncol(x$train)],center = models$xm,scale = models$scales)%*%models$coef +models$ym
  trainMSE<-apply(predictionsTrain,MARGIN=2,MSE,y=train$quality)
  
  bestRidgeModel<-list(bestMse=min(validationMSE),bestModel=which.min(validationMSE))
  mceValidation<-mean( (predictionsValidation[,bestRidgeModel$bestModel]>GOOD_WINE)!=(validation$quality>GOOD_WINE))
    
  list(trainMSE=trainMSE, validationMSE= validationMSE, model=models,bestRidgeModel=bestRidgeModel,mceValidation=mceValidation)
})
```

```{r, echo=FALSE}
plot(ridge.models$poly3$trainMSE,type="l",ylim = c(0.35,0.58))
lines(ridge.models$poly3$validationMSE,type="l")

plot(ridge.models$poly2$trainMSE,type="l",ylim = c(min(ridge.models$poly2$trainMSE),max(ridge.models$poly2$validationMSE)))
lines(ridge.models$poly2$validationMSE,type="l")

plot(ridge.models$pca6$trainMSE,type="l",ylim = c(min(ridge.models$pca6$trainMSE),max(ridge.models$pca6$validationMSE)))
lines(ridge.models$pca6$validationMSE,type="l")


```


```{r,eval=FALSE, echo=FALSE}
#Learninng rate
#TODO Make a function
#plot.learning.rate<-function(model,data,min,max){
#  set.seed(23)
#  
#  
#}
set.seed(23)
n<-seq(500,4000, by=100)
mse.train<- vector()
mse.validation<-vector()
for(i in n){
  temp.mse.train<-0
  temp.mse.validation<-0
  for(j in 1:100){
    k<- sample(x = 1:nrow(wine),size =i,replace = FALSE)
    subset <- wine[k,]
    s<- sample.split(subset, SplitRatio = 2/3)
    trainSubset<- subset[s==TRUE,]
    validationSubset<-subset[s==FALSE,]
    mod<-lm(quality~.^3,data=trainSubset)
    temp.mse.train<-mean((predict(mod)-trainSubset$quality)^2)+temp.mse.train
    temp.mse.validation<-mean((predict(mod,newdata=validationSubset)-validationSubset$quality)^2)+     temp.mse.validation
    
  }
  mse.train = c(mse.train,temp.mse.train/100)
  mse.validation=c(mse.validation,temp.mse.validation/100)

}

plot(x=n,y=mse.validation,type="l",col="blue",ylim=c(min(mse.train),min(c(2,max(mse.validation)))))
lines(x=n,y=mse.train,type="l", col="red")
```

The fact that the linear coefficients do not quickly shrink towards zero even when we penalize their absolute value further reinforces our belief that we should not remove attributes.

```{r,cache=TRUE,eval=FALSE}
require(fields)
tps <- lapply(dataSets, function(x){
  x<-dataSets$pca4
#  train <- as.matrix(as.data.frame(x$train))
#  model<-fastTps(x = as.matrix(x$train),Y = as.matrix(x$train$quality),theta = #1,find.trA = TRUE)
  model <- Tps(train[,-ncol(train)], as.data.frame(x$train)$quality, scale.type= "unscaled")
  pred <- predict(model, x = as.matrix(as.data.frame(x$validation)))
  list(validationMSE=MSE(pred,validation[12]),trainMSE=mean(model$residuals^2),model=model,mceValidation=mean((pred>GOOD_WINE)!=(validation$quality>GOOD_WINE)))
})

```

```{r,eval=TRUE,cache=TRUE}
require(randomForest)
randomForest.models <- lapply(dataSets, function(x){
  model<-randomForest(quality ~ ., data=x$train, ntree = 40)
  tuned.model <- randomForest(quality ~ ., data=x$train, ntree =
                                which.min(model$mse))
  pred<-predict(model,as.data.frame(x$validation))
  list(validationMSE=MSE(pred,validation$quality),trainMSE=tuned.model$mse[length(tuned.model$mse)])
})
```

```{r}
mse<- sapply(randomForest.models,function(x){
  x$validationMSE
})
plot(mse)
```
LOG?

```{r,eval=FALSE,cache=TRUE}
distMatrix=function(N,M){
  eN<-matrix(1,nrow=nrow(N),ncol=ncol(N))
  eM<-matrix(1,nrow=nrow(M),ncol=ncol(N))
  sqrt(eM%*%t(N^2)-2*M%*%t(N)+M^2%*%t(eN))
}

temp.dataSets <- dataSets
temp.dataSets$pca1 <- NULL

loc.reg.mse <- lapply(temp.dataSets, function(x){
# PCA1
  matrix <- as.matrix(as.data.frame(x$train))
  distance<-distMatrix(matrix[,-ncol(matrix)],as.matrix(as.data.frame(x$validation)))
  pred <- c()
  validationMSE <- c()
  for (r in seq(30, 90, 30)){
    for(i in 1:nrow(validation)){
      n<-sort(distance[i,],index.return = TRUE)$ix[1:r]
      mod<-lm(quality~.,data=as.data.frame(x$train)[n,])
      pred[i]<-predict(mod,as.data.frame(x$validation)[i,])}
      validationMSE <- c(validationMSE, MSE(pred,validation[12]))}
  list(validationMSE=validationMSE)
})

plot(c(30,60,90), loc.reg.mse$normal$validationMSE, type = "l", xlab = "Number of neighbors", ylab = "validation MSE", col = 6, xlim = c(30, 100))
for (i in 2:11){
  lines(c(30,60,90), loc.reg.mse[[i]]$validationMSE, col = i + 5)}

legend("topright", c("normal","poly2","poly3","PCA","PCA10","PCA9","PCA8","PCA7","PCA6","PCA5","PCA4","PCA3","PCA2"), col=seq(6,16), lty=1, cex = 0.5)
```

#5.x Regression models for classification
In a way to establish a new better baseline for the classification part. We decided to try our regression models for classification.
```{r,eval=FALSE}
#Linear
linear.mce<-sapply(linear.models,function(x){
  x$mceValidation
})
barplot(linear.mce,las=2,main="Linear models",ylim = c(0,0.45))

#Lasso
lasso.mce<-sapply(lasso.models,function(x){
  x$bestMceValidation
})
barplot(lasso.mce,las=2,main="Lasso models",ylim = c(0,0.45))

#Ridge
ridge.mce<- sapply(ridge.models,function(x){
  x$mceValidation
})
barplot(ridge.mce,las=2,main="Ridge models",ylim = c(0,0.45))


```


##5.2 CLASSIFICATION
	
```{r, echo = FALSE}
MCE <- function(x, y)
  {mean(x != y)}
```
	
```{r, echo = FALSE}
MIN_PRECISION=0.75
validation$binary.quality <- validation$quality >= 6
validation$quality=NULL
validation$binary.quality<-as.factor(validation$binary.quality)

for(i in 1:length(dataSets)){
  dataSets[[i]]$train$binary.quality<-(dataSets[[i]]$train$quality>GOOD_WINE)
  dataSets[[i]]$train$binary.quality<-as.factor(dataSets[[i]]$train$binary.quality)
  dataSets[[i]]$train$quality<-NULL
}

baseline.misclass <- MCE(TRUE, validation$binary.quality)
baseline.misclass
```

What is KNN...

```{r,eval=FALSE, warning = FALSE}
require(FNN)
knn.models<- lapply(dataSets,function(x){
  mce<-sapply(c(3,4,5,6,10,20,100), function(k){
    pred<-knn(train = x$train[-ncol(x$train)],test = x$validation,cl = x$train$binary.quality,k=k)
    MCE(pred,validation$binary.quality)
  })
  precisi  
  list(mce=mce,best=list(bestMce=min(mce)),k=c(3,4,5,6,10,20,100)[which.min(mce)])
})

mce<-sapply(knn.models, function(x){
  x$best$bestMce
})
# barplot(mce,las=2)
# plot, y = validation MCE, x = antal grannar
```

Poly3 with K = 5 is best (validation MSE: 0.1935047).

Describe LDA and QDA.

```{r,eval=FALSE}
require(MASS)

# remove poly2 and poly3?

lda.qda.models <- lapply(dataSets, function(x){
  wine.lda.cv <- lda(binary.quality ~ ., data = x$train, CV = TRUE)
  wine.qda.cv <- qda(binary.quality ~ ., data = x$train, CV = TRUE)
  wine.lda <- lda(binary.quality ~ ., data = x$train)
  wine.qda <- qda(binary.quality ~ ., data = x$train)

  list(CV.MCE.lda=MCE(wine.lda.cv$class, x$train[,ncol(x$train)]),CV.MCE.qda=MCE(wine.qda.cv$class,
  x$train[,ncol(x$train)]),qda.validation.MCE=MCE(predict(wine.qda, x$validation)$class,
  validation[,12]),lda.validation.MCE=MCE(predict(wine.lda, x$validation)$class, 
  validation[,12]))
})

# barplot
# $posterior + ROCR...
```

SVM...

```{r,eval=FALSE}
require(e1071)

#Hyperparameters
degree<-c(3)
gamma<-c(0.10)
coef0<-c(1)
cost<-c(1)

#Models for diffrent kernels
models<-apply(expand.grid(degree,gamma,coef0,cost),MARGIN = 1,function(x){
  svm(binary.quality~.,data=dataSets$normal$train,kernel="polynomial",degree=x[1],gamma=x[2],coef0=x[3],cost=x[4])
})
models<-c(models,lapply(cost,function(x){
  svm(binary.quality~.,data=dataSets$normal$train,kernel="linear",cost=x[1])
}))
models<-c(models,apply(expand.grid(gamma,cost),MARGIN = 1,function(x){
  svm(binary.quality~.,data=dataSets$normal$train,kernel="radial",gamma=x[1],cost=x[2])
}))
models<-c(models,apply(expand.grid(gamma,coef0,cost),MARGIN = 1,function(x){
  svm(binary.quality~.,data=dataSets$normal$train,kernel="sigmoid",gamma=x[1],coef0=x[2],cost=x[3])
}))

#Use the models to predict
df<-as.data.frame(t(sapply(models,function(x){validationError<-mean((predict(object = x,newdata=dataSets$normal$validation))!=(validation$binary.quality))
trainError<-mean((predict(object = x))!=(dataSets$normal$train$binary.quality))
list(kernel = x$call$kernel,cost=x$cost,gamma=x$gamma,coef0=x$coef0,degree=x$degree,trainError=trainError,validationError=validationError )
  })))


View(df[order(as.numeric(df$validationError)),])

# barplot 
```	


```{r,eval=FALSE, cache=TRUE}
set.seed(56)
require(randomForest)
randomForest.c.models <- lapply(dataSets, function(x){
  model<-randomForest(binary.quality ~ ., x$train, ntree = 5)
  tuned.model <- randomForest(binary.quality ~ ., x$train, ntree =
                                which.min(model$err.rate[,1]))
  pred<-predict(tuned.model,as.data.frame(x$validation))
  list(validationMCE=MCE(pred,validation$binary.quality))
})
#plot?
```

```{r,eval=FALSE}
set.seed(56)
require(ada)

dataSets.new <- dataSets
dataSets.new$pca1 <- NULL

ada.models <- lapply(dataSets.new, function(x){
  model<-ada(x$train[,1:ncol(x$train)-1], x$train[,ncol(x$train)], iter = 10)
  # tune?
  pred<-predict(model,x$validation)
  list(validationMCE=MCE(pred,validation$binary.quality))
})
#min mce
```

```{r,eval=FALSE}
require(ROCR)

# returnerna model
logistic.reg <- lapply(dataSets, function(x){
model <- glm(binary.quality ~ ., data = x$train, family = "binomial")
pred <- predict.glm(model, newdata = x$validation, type = "response")
pred.rocr <- prediction(pred, validation$binary.quality)
})

c.plot <- function(x){
  for (j in c("acc","prec","rec")){

    plot(performance(x$normal, j), col = 1, xlim = c(0,1.2), lwd = 3)
    for (i in 2:length(x)){
      lines(performance(x[[i]], j)@x.values[[1]], performance(x[[i]],
        j)@y.values[[1]], col = i, lwd = 3)}
    legend("topright", names(x), col=seq(1,length(x)), lty=1, cex = 0.5)}
  
  
  plot(performance(x[[1]], "acc")@x.values[[1]], performance(x[[1]], "acc")@y.values[[1]] +
  performance(x[[1]], "prec")@y.values[[1]], col = 1, xlim = c(0,1.2), ylim = c(1.30,1.65), type = 
  "l", xlab = "Cutoff", ylab = "Accuracy + Precision", lwd = 3)
  
  for (i in 2:length(x)){
    lines(performance(x[[i]], "acc")@x.values[[1]], performance(x[[i]], "acc")@y.values[[1]] +
    performance(x[[i]], "prec")@y.values[[1]], col = i, lwd = 3)}
  legend("topright", names(x), col=seq(1,length(x)), lty=1, cex = 0.5)
}

```
##Results and disc
best model best (lowest mce) feature set.


diskussion om att de nödvändigtvis inte har fått bäst modell efter att de varierat...!


