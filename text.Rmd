---
title: "Project"
author: "Simon Almerström Przybyl & Daniel Öhman"
date: "July 27th 2015"
output: html_document
---

#1. INTRO / EXPLAIN DATA SET AND TASK

For our project, we have chosen to examine a wine data set. The data set contains measures of chemical levels in wine as its input variables, and the quality of wine as its target variable. The measured chemical levels ranged from the pH level of the wine to the alcohol percentage of the wine, in total 11 different chemical levels were measured on a continuous linear scale. The quality of the wine was measured on a discrete scale from 0 to 10, though the actual observed range was only from 3 to 9. It should also be noted that the quality of the wine was graded by letting individuals taste the wine, the quality scale therefore reflects expert opinion.

```{r}
#download.file(url="https://archive.ics.uci.edu/ml/machine-learning-databases/wine-qual#ity/winequality-white.csv",destfile = "winequality-white.csv")
wine<-read.csv("winequality-white.csv",sep=";")
str(wine)
```

Our task is thus to investigate how different chemical levels affect the quality of the wine, and we split this task into two subtasks:

1 - We view the quality of wine as a continuous variable allowed to take any value on the interval [0,10], this view of the problem yields a regression problem.

2 - We define a good wine to be a wine having quality larger than or equal to 6, and a poor wine to be a wine having quality strictly less than 6. This yields a binary classification problem of separating good wine from poor wine.

Regarding the accuracy of the different subtasks:

In the regression subtask we are of course seeking to minimize the MSE as far as possible. Moreover, we definitely want our regression model to give a lower MSE than the MSE we obtain in our baseline model, this model consists of simply guessing the mean quality of the training.

```{r}
hist(train$quality,col = "red",main = "Training", xlab = "Quality")
```

Note that the histogram plots the quality of the training observations, we will describe the process of splitting the data into training, validation and test sets more thorughly later.

Since our training data is approximately normally distrubuted around 6 (with of variance of 0.7723022), our baseline model will perform quite well (this model yields a validation MSE of 0.7705384 and a test MSE of 0.7779192). Therefore, in order to justify the existence of our more sophisticated models, we want them to give a much lower MSE.

How low we want to push down the MSE for us to be satisfied with our models is a difficult question and depends on how are models are supposed to be used. We believe a resonable purpose for our models is to act as a tool for beverage stores when they decide which wine they should keep in stock. However, it is still difficult to define how low the MSE should be for us to be content: When one is for example constructing a bridge, one will probably want the prediction of the target value to be precise enough so the probability of the bridge collapsing is negligent. In our case, our regression models can not really be said to have such a clear goal: We can basically only tell if one given model is better than another given model, not if they are good enough in an absolute sense.

Given the discussion above, any quantitative measure of how much we want to reduce our test MSE is completely arbitrary. However, we pretend that the marketing department of the beverage store has demanded that the test MSE of a sufficient model should be 50% lower than the test MSE of our basline model, such a reduction in MSE would let the marketing department advertise our model as being "50% better than chance".

In the classification subtask, our main interest is to have a high precision rate, intuitively because we seek to minimize the risk of someone intending to buy a good wine but actually getting a poor wine. We believe a 90% precision rate is sufficiently high for this purpose.


#2. DATA CLEANING

We downloaded the data set from the UCI Machine Learning Repository [link]( https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/). At the repository, data regarding both white and red wine was available in two separate files, however we chose to only use the data for the white wine since we believe that the quality of wine with different colours may be affected differently by varying chemical levels. Moreover, expert opinion varies when judging white wine compared to red wine.

The data set did not contain any unknown values. Initially, we did not search for any outliers but after having constructed a few models, we noticed that our validation MSE was systematically lower than our training MSE. When this was noticed, we tried to resplit our data into training, validation and test sets a few times, however the pecularity of obtaining lower validation MSE than training MSE was present for most of the splits. The pecularity was especially pronounced for the simpler models. This lead us to believe that there were outliers present in our data: We believe that having outliers present in the training data will not affect the overall behavior of simpler models but will push up the training MSE highly for these models (since the prediction for the outliers will be far off).

We searched for outliers (i.e. observations deviating extremely) by examining the Euclidean norm of the attributes of the observations:

```{r}
require(scales)
wine <- cbind(as.data.frame(scale(wine[,-12],center = TRUE)),quality=wine[,12])
euc <- sqrt(wine[,1]^2+wine[,2]^2+wine[,3]^2+wine[,4]^2+wine[,5]^2+wine[,6]^2+wine[,7]^2+wine[,8]^2+wine[,9]^2+wine[,10]^2+wine[,11]^2)
plot(sort(euc),type="l", xlab = "Sorted index", ylab = "Euclidean norm")
sorted<-sort(euc, index.return = TRUE)$ix
wine<-wine[-sorted[c(1:80,(length(sorted)-200):length(sorted))],]
euc <- sqrt(wine[,1]^2+wine[,2]^2+wine[,3]^2+wine[,4]^2+wine[,5]^2+wine[,6]^2+wine[,7]^2+wine[,8]^2+wine[,9]^2+wine[,10]^2+wine[,11]^2)
lines(y=sort(euc),x=51:(nrow(wine)+50), col = alpha("red", 0.5), lw=6)
```

The black line shows the Euclidean norm of all of the observations in the wine data set, ranging from the one with the lowest norm to the one with the highest. Since there are clearly a few observations deviating extremely in terms of their Euclidean norm, we chose to remove these observations and only keep the line segment marked in red. The following histogram shows the quality distribution of the removed observations:

```{r}
hist(wine$quality[sorted[c(1:80, (length(sorted)-200):length(sorted))]], col = "red", main = "Removed data", xlab = "Quality")
```

The histogram shows that the removed observations did not come from any particular quality class (they are distributed similarly to the training data set).

RUBRIK

The wine data set was split into training, validation and test sets by doing a random 64-16-20 split while preserving relative ratios of different labels of the target variable (note that this split is done by first doing a 80-20 split into a combined training-validation set and test set, followed by a 80-20 split of the combined training-validation set). For the binary classification we created a new target variable which is either true or false depending on if the wine is good or poor. Note that the same data split was used for both the regression problem and the classification problem. Finally, we scaled and centered all of the attributes except for the target variable.

```{r}
library(caTools)
set.seed(24)
s <- sample.split(Y = wine$quality,SplitRatio = 0.8)
test <- wine[s==FALSE,]
temp <-wine[s==TRUE,]
s <- sample.split(Y = temp$quality,SplitRatio = 0.8)
validation <- temp[s==FALSE,]
train <- temp[s==TRUE,]
```

#3 & 4. FEATURE IMPORTANCE SCORES / PCA

We used PCA to find the successive orthogonal directions of maximum variance:

```{r}
PCA <- prcomp(train[,-12], center = FALSE, scale. = FALSE)
PCA$sdev
```

Intuitively PCA defines a set of new base vectors of our data (corresponding to a rotation) and also gives the standard deviations of our data in these directions, the idea is to keep the directions of high variance since these are assumed to be the most informative (at least given a linear relation between the attrbiutes and the target variable).

Since the standard deviations decrease quite slowly without any sharp decline, except for the step from the 10th to the 11th principle component, there is not any natural cut-off point for how many principle components we should choose. We therefore choose to express our data using the first i principle components, for every i from 1 to 11, and create a new data frame for each such transformation.

We also create new data frames where we have projected our original data onto 2nd and 3rd order polynomials, note that these polynomials contain 11 variables and thus have a lot of cross terms. We do not project onto polynomials of higher order because of computational limitations, and we do not believe that such polynomials would yield better models since they would most likely overfit to the training data. When we create models, we will simultaneously create a given model (for example linear regression) for all of these data frames.

```{r}
#Create data set with diffrent attribute
trainPoly<-lapply(2:3,function(x){
  foo<- with(train,polym(fixed.acidity,volatile.acidity,citric.acid,residual.sugar,chlorides,free.sulfur.dioxide,total.sulfur.dioxide,density,pH,sulphates,alcohol,degree = x,raw=TRUE))
foo<-as.data.frame(foo)
foo$quality=train$quality
list(foo)
})

validationPoly<-lapply(2:3,function(x){
  foo<- with(validation,polym(fixed.acidity,volatile.acidity,citric.acid,residual.sugar,chlorides,free.sulfur.dioxide,total.sulfur.dioxide,density,pH,sulphates,alcohol,degree = x,raw=TRUE))
  foo<-as.data.frame(foo)
  list(foo)
})


PCA <- prcomp(train[,-12], center = FALSE, scale. = FALSE)
PCA.validation<-lapply(1:11, function(i){
    if (i == 1){
    foo <- as.matrix(validation[,-12]) %*% PCA$rotation[,1:i]
    list(data.frame(PC1 = foo))
  } else {
     list(as.data.frame((as.matrix(validation[,-12]) %*%
                                         PCA$rotation)[,1:i]))
  }
})

PCA.train<-as.data.frame(cbind(PCA$x,quality=train$quality))

dataSets<- list(normal=list(train=train,validation=validation),
                poly2=list(train=trainPoly[[1]],validation=validationPoly[[1]]),
                poly3=list(train=trainPoly[[2]],validation=validationPoly[[2]]),
                pca=list(train=PCA.train,validation=PCA.validation[[11]]),
                pca10=list(train=PCA.train[,c(1:10,12)],validation=PCA.validation[[10]]),
                pca9=list(train=PCA.train[,c(1:9,12)],validation=PCA.validation[[9]]),
                pca8=list(train=PCA.train[,c(1:8,12)],validation=PCA.validation[[8]]),
                pca7=list(train=PCA.train[,c(1:7,12)],validation=PCA.validation[[7]]),
                pca6=list(train=PCA.train[,c(1:6,12)],validation=PCA.validation[[6]]),
                pca5=list(train=PCA.train[,c(1:5,12)],validation=PCA.validation[[5]]),
                pca4=list(train=PCA.train[,c(1:4,12)],validation=PCA.validation[[4]]),
                pca3=list(train=PCA.train[,c(1:3,12)],validation=PCA.validation[[3]]),
                pca2=list(train=PCA.train[,c(1:2,12)],validation=PCA.validation[[2]]),
                pca1=list(train=PCA.train[,c(1,12)],validation=PCA.validation[[1]]))
                
dataSets$normal$validation$quality=NULL
```

We examine the linear dependency between the attributes and return those who are most linearly dependent (those with largest absolute value of correlation):

```{r}
#cor
cor_wine<-cor(train)
cor_wine[lower.tri(cor_wine,diag=TRUE)] <-NA
df_wine <- as.data.frame(as.table(cor_wine))
df_wine <- df_wine[order(-abs(df_wine$Freq)),]
#Top 10 highest correlated attributes
colnames(df_wine)<- c("Var1","Var2","Cor")
df_wine[1:10,]
```

If we were to remove some features, it would be reasonable to remove for example residual.sugar and total.sulfur.dioxide since density already provides information about these attributes.

KOMMENTAR.

We also examine the mutual information criterion:

```{r}
#Shared information
require(plyr)
require(reshape)
require(entropy)
my_fun<-function(x){freq <- count(train,c("quality",x))
colnames(freq) <- c("Var1","Var2","Freq")
freq2d<-cast(freq, Var1 ~ Var2,fill=0,value ="Freq")
mi.plugin(freqs2d = freq2d)}

cn<- colnames(wine[,-12])
mi <- sapply(cn,my_fun)
sort(mi,decreasing = TRUE)
```

The fact that density has the highest mutual information criterion intuitively matches our results about the linear dependency of the attributes: By using the linear dependencies, knowing the density of wine provides us with information about its residual.sugar, alcohol and total.sulfur.dioxide values. Knowing four attributes, we most likely have a higher probability of knowing the quality of the wine than if we only know one attribute.

#5. MODELS

##5.1 REGRESSION

```{r}
MSE <- function(x,y){
  mean((x - y)^2)}
```

The first model we construct is simply linear regression:

```{r}
linear.mse <- lapply(dataSets, function(x){
  model<-lm(quality~.,data=as.data.frame(x$train))
  pred<-predict(model,as.data.frame(x$validation))
  list(validationMSE=MSE(pred,validation[12]),
  trainMSE=mean(model$residuals^2))
})
```

```{r}
require(lars)

lasso.models <- lapply( dataSets, function(x){
  foo<-as.data.frame(x$train)
  bar<- as.data.frame(x$validation)
  m<-as.matrix(foo[-ncol(foo)])
  models<-lars(x = m, y = as.numeric(foo$quality), type = "lasso")
  pred<-predict(models, as.matrix(as.data.frame(bar)))
  mse<-sapply(1:ncol(pred$fit),function(x){MSE(pred$fit[,x],validation$quality)})
  names(x)=list(models=models,mse=mse)
})
```

The highest mutual information between the target variable and the attributes was observed for the attribute density, therefore it seems reasonable to try to construct a model where we predict quality using only density. The plot below shows the relationship between density and quality:

```{r}
plot(train$density, train$quality)
```

The only relationship which seems clear is a slight negative linear tendency, we therefore regress quality on density linearly:

```{r}
lin.model2 <- lm(quality ~ density, data = train)
lin.pred2 <- predict(lin.model2, newdata = validation)
MSE(lin.pred2, validation[,12])
```

PCA LM

```{r}
PCA <- prcomp(train[,-12], center = FALSE, scale. = FALSE)  
  
PCA.lm.mse <- c()
for (i in 1:11){
  if (i == 1){
    PCA.train <- as.data.frame(cbind(PC1 = PCA$x[,1:i], quality = train[,12]))
  } else {
    PCA.train <- as.data.frame(cbind(PCA$x[,1:i], quality = train[,12]))
  }
  PCA.lm <- lm(quality ~ ., data = PCA.train)
  if (i == 1){
    PCA.validation <- as.matrix(validation[,-12]) %*% PCA$rotation[,1:i]
    PCA.validation <- data.frame(PC1 = PCA.validation)
  } else {
    PCA.validation <- as.data.frame((as.matrix(validation[,-12]) %*%
                                       PCA$rotation)[,1:i])
  }
  PCA.lm.mse[i] <- MSE(predict(PCA.lm, PCA.validation), validation[,12])
}
```

Polynomial regression
2-degree polynomial

```{r}
lin.model3 <- lm(quality ~ .^2, data = train)
lin.pred3 <- predict(lin.model3, newdata = validation)
MSE(lin.pred3, validation[,12])
```

3-degree polynomial

```{r}
lin.model4 <- lm(quality ~ .^3, data = train)
lin.pred4 <- predict(lin.model4, newdata = validation)
MSE(lin.pred4, validation[,12])
```

We also construct a set of lasso models, these models give a penalty on the square of the linear coefficients, a parameter lambda decides the weight of the penalty. These models effectively corresponds to successively removing our attributes (as lambda grows larger) and might thus give us information regarding if some of our attributes are redundant:

```{r}
require(lars)
lasso.models <-lars(x = as.matrix(train[,-12]), y = as.vector(train[,12]), type = "lasso")
plot(lasso.models)

lasso.mse <- sapply(1:14,function(x){MSE(predict(lasso.models,as.matrix(validation[,-12]))$fit[,x], validation[,12])})
plot(x=1:14,y=lasso.mse,xlab="Model")
```

The more attributes that were removed, the higher the MSE became, this fact supports our argument for keeping most of our attributes. Moreover, we also constructed ridge models which penalize the absolute value of the linear coefficients and similar results were observed for these models:

```{r}
library(MASS)

trainPoly<-lapply(1:3,function(x){
  foo<- with(train,polym(fixed.acidity,volatile.acidity,citric.acid,residual.sugar,chlorides,free.sulfur.dioxide,total.sulfur.dioxide,density,pH,sulphates,alcohol,degree = x,raw=TRUE))
foo<-as.data.frame(foo)
foo$quality=train$quality
list(foo)
})

validationPoly<-lapply(1:3,function(x){
  foo<- with(validation,polym(fixed.acidity,volatile.acidity,citric.acid,residual.sugar,chlorides,free.sulfur.dioxide,total.sulfur.dioxide,density,pH,sulphates,alcohol,degree = x,raw=TRUE))
foo<-as.data.frame(foo)
list(foo)
})

lambda<- seq(0, 2500,by = 100)
ridge.models<- lapply(1:3,function(x){lm.ridge(quality~.,data=as.data.frame(trainPoly[[x]]),lambda = lambda)})

pred.ridge.train <- lapply(1:3, function(x){scale(as.data.frame(trainPoly[[x]])[-ncol(as.data.frame(trainPoly[[x]]))],center = TRUE, scale = ridge.models[[x]]$scales)%*%ridge.models[[x]]$coef + ridge.models[[x]]$ym})

pred.ridge.validation <- lapply(1:3, function(x){scale(as.data.frame(validationPoly[[x]]),center = TRUE, scale = ridge.models[[x]]$scales)%*%ridge.models[[x]]$coef + ridge.models[[x]]$ym})

mse.ridge.train<-lapply(1:3,function(x){apply(pred.ridge.train[[x]],MARGIN = 2,FUN = function(x){mean((train[12]-x)^2)})})

mse.ridge.validation<-lapply(1:3,function(x){apply(pred.ridge.validation[[x]],MARGIN = 2,FUN = function(x){mean((validation[12]-x)^2)})})

plot(y=mse.ridge.train[[3]],x=lambda,type="l",ylim=c(0.3,0.7))
lines(y=mse.ridge.validation[[3]],x=lambda)

plot(ridge.models[[3]])

#Learninng rate
#TODO Make a function
#plot.learning.rate<-function(model,data,min,max){
#  set.seed(23)
#  
#  
#}
set.seed(23)
n<-seq(500,4000, by=100)
mse.train<- vector()
mse.validation<-vector()
for(i in n){
  temp.mse.train<-0
  temp.mse.validation<-0
  for(j in 1:100){
    k<- sample(x = 1:nrow(wine),size =i,replace = FALSE)
    subset <- wine[k,]
    s<- sample.split(subset, SplitRatio = 2/3)
    trainSubset<- subset[s==TRUE,]
    validationSubset<-subset[s==FALSE,]
    mod<-lm(quality~.^3,data=trainSubset)
    temp.mse.train<-mean((predict(mod)-trainSubset$quality)^2)+temp.mse.train
    temp.mse.validation<-mean((predict(mod,newdata=validationSubset)-validationSubset$quality)^2)+     temp.mse.validation
    
  }
  mse.train = c(mse.train,temp.mse.train/100)
  mse.validation=c(mse.validation,temp.mse.validation/100)

}

plot(x=n,y=mse.validation,type="l",col="blue",ylim=c(min(mse.train),min(c(2,max(mse.validation)))))
lines(x=n,y=mse.train,type="l", col="red")
```

The fact that the linear coefficients do not quickly shrink towards zero even when we penalize their absolute value further reinforces our argument for not removing attributes.

```{r}
# thin plate spline
  
  d <- function(X,Y){
    sqrt(sum((X - Y)^2))}
  
  f <- function(X,j){
    if (d(X, train[j, 1:11]) > 0){(d(X, train[j, 1:11])^2)*log(d(X, train[j, 1:11]))}
    else {0}}
  
  projections <- matrix(nrow = length(train[,1]), ncol = length(train[,1]))
  
#  for (i in 1:length(train[,1])){
#    for (j in 1:length(train[,1])){
#      projections[i,j] <- f(train[i, 1:11], j)
#    }}
  
#  projections.data <- data.frame(cbind(projections, quality = train$quality))
  
#  ridge.model <- lm.ridge(quality ~ ., data = projections.data, lambda = 1)
  
#  ridge.betas <- coef(ridge.model)

# applicera nedanstående funktion!

  f.ridge <- function(X){
    v <- c(ridge.betas[1])
    for (i in 1:length(train[,1])){v[i+1] <- ridge.betas[i+1]*f(X,i)}
    sum(v)}
  
#  new.vector <- c()
#  for (i in 1:length(validation[,1])){
#    new.vector[i] <- f.ridge(validation[i,1:11])}
# slut thin plate spline
```

```{r}
require(fields)

PCA <- prcomp(train[,-12], center = FALSE, scale. = FALSE) 
  
thin.plate.PCA <- c()  
for (i in 1:11){
  if (i == 1){PCA.validation <- as.matrix(validation[,-12]) %*% PCA$rotation[,1:i]
  PCA.validation <- as.matrix(data.frame(PC1 = PCA.validation))
  } else {PCA.validation <- (as.matrix(validation[,-12]) %*% PCA$rotation)[,1:i]}
  thin.plate <- Tps(PCA$x[,1:i], train[,12])
  thin.plate.PCA[i] <- MSE(predict(thin.plate, x = PCA.validation), validation[,12])}

for (i in c(0.001, 0.01, 0.1, 1, 5, 10)){
PCA.validation <- (as.matrix(validation[,-12]) %*% PCA$rotation)[,1:5]
thin.plate <- Tps(PCA$x[,1:5], train[,12], lambda = i)
print(MSE(predict(thin.plate, x = PCA.validation), validation[,12]))}

```

```{r}
require(randomForest)

rf.reg1 <- randomForest(quality ~ ., data = train, ntree = 10)
rf.reg1.tuned <- randomForest(quality ~ ., data = train, ntree = which.min(rf.reg1$mse))
pred <- predict(rf.reg1.tuned, validation[,-12])
rf.mse <- MSE(pred, validation[,12])
rf.mse

rf.reg2 <- randomForest(log(quality) ~ ., data = train, ntree = 20)
rf.reg2.tuned <- randomForest(log(quality) ~ ., data = train, ntree = which.min(rf.reg2$mse))
pred <- predict(rf.reg2.tuned, validation[,-12])
rf.mse <- MSE(exp(pred), validation[,12])
rf.mse

# PCA RF

PCA <- prcomp(train[,-12], center = FALSE, scale. = FALSE)

rf.mse.PCA <- c()

for (i in 1:11){

  if (i == 1){
    PCA.train <- as.data.frame(cbind(PC1 = PCA$x[,1:i], quality = train[,12]))
  } else {
    PCA.train <- as.data.frame(cbind(PCA$x[,1:i], quality = train[,12]))
  }
  rf.reg2 <- randomForest(log(quality) ~ .^3, data = PCA.train, ntree = 15)
  rf.reg2.tuned <- randomForest(log(quality) ~ .^3, data = PCA.train, ntree = which.min(rf.reg2$mse))

  if (i == 1){PCA.validation <- as.matrix(validation[,-12]) %*% PCA$rotation[,1:i]
    PCA.validation <- data.frame(PC1 = PCA.validation)
  } else {
    PCA.validation <- as.data.frame((as.matrix(validation[,-12]) %*%
    PCA$rotation)[,1:i])}

  pred <- predict(rf.reg2.tuned, PCA.validation)
  rf.mse.PCA[i] <- MSE(exp(pred), validation[,12])
}
plot(rf.mse.PCA)
min(rf.mse.PCA)
```

```{r}
loc.reg <- loess(quality ~ ., data = PCA.train, degree = 1)
loc.reg.mse <- MSE(predict(loc.reg, newdata = PCA.validation)[-1285], validation[-1285,12])

distance.fct <- function(){

  distlist <- list()
  for (i in 1:length(validation[,1])){

    distance <- c()
    for (j in 1:length(train[,1])){
      distance[j] <- sum((validation[i,-12] - train[j,-12])^2)}

    distlist[[i]] <- distance
    print(i/length(validation[,1]))}
  
  write.table(distlist, file = "distlist.txt", row.names = FALSE, col.names = FALSE)}

distlist <- read.table("distlist.txt")

local.regression <- function(k, d){
  
  pred <- c()
  
  for (i in 1:length(validation[,1])){
  
    distance <- distlist[[i]]
    closest.x <- sort(distance, index.return=TRUE)$ix[1:k]
    df <- data.frame(train[closest.x,])

    loc.poly <- lm(quality ~ poly(fixed.acidity, volatile.acidity, citric.acid, residual.sugar,
    chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol, degree
    = d, raw = TRUE), data = df)

    pred[i] <- (loc.poly$coefficients[1] + loc.poly$coefficients[2]*validation[i,1] +
    loc.poly$coefficients[3]*validation[i,2] + loc.poly$coefficients[4]*validation[i,3] + 
    loc.poly$coefficients[5]*validation[i,4] + loc.poly$coefficients[6]*validation[i,5] +
    loc.poly$coefficients[7]*validation[i,6] + loc.poly$coefficients[8]*validation[i,7] +
    loc.poly$coefficients[9]*validation[i,8] + loc.poly$coefficients[10]*validation[i,9] +
    loc.poly$coefficients[11]*validation[i,10] + loc.poly$coefficients[12]*validation[i,11])}

  remove <- which(is.na(pred))

  if (is.na(remove[1])) {mse.loc.reg <- MSE(pred, validation[,12])}
  if (is.na(remove[1]) == FALSE) {mse.loc.reg <- MSE(pred[-remove], validation[-remove,12])}

return(list(mse.loc.reg))}

test <- matrix(ncol = 2, nrow = 490)
for (i in seq(30,500, 20)) {
  
  for (j in c(1,2)) {
    test[i,j] <- local.regression(i, j)[[1]]}}

```

##5.2 CLASSIFICATION
	
```{r}
MCE <- function(x, y)
  {mean(x != y)}
```
	
```{r}
train$binary.quality <- train$quality >= 6
validation$binary.quality <- validation$quality >= 6
test$binary.quality <- test$quality >= 6
train$quality=NULL
test$quality=NULL
validation$quality=NULL
train$binary.quality<- as.factor(train$binary.quality)
validation$binary.quality<-as.factor(validation$binary.quality)
test$binary.quality<- as.factor(test$binary.quality)

baseline.misclass <- MCE(TRUE, validation$binary.quality)
baseline.misclass
```


```{r}
require(FNN)
wine.knn.misclass <- c()

for(i in 1:25){
  wine.knn <- knn(train = train[,1:11], test = validation[,1:11], k = i, cl = train[,12])
  wine.knn.misclass[i] <- MCE(wine.knn, validation[,12])
}
plot(wine.knn.misclass)
```

```{r}
require(MASS)

# cross val

wine.lda.cv <- lda(binary.quality ~ ., data = train, CV = TRUE)
wine.qda.cv <- qda(binary.quality ~ ., data = train, CV = TRUE)

cv.misclass.error.lda <- MCE(wine.lda.cv$class, train[,12])
cv.misclass.error.qda <- MCE(wine.qda.cv$class, train[,12])

# end cross val

wine.lda <- lda(binary.quality ~ ., data = train)
wine.qda <- qda(binary.quality ~ ., data = train)

qda.validation.mce <- MCE(predict(wine.qda, validation[,-12])$class, validation[,12])
lda.validation.mce <- MCE(predict(wine.lda, validation[,-12])$class, validation[,12])
```

```{r}
require(e1071)

#Hyperparameters
degree<-c(3,4)
gamma<-c(0.01,0.05,0.10,0.15)
coef0<-c(1,2)
cost<-c(1,2,3)

#Models for diffrent kernels
models<-apply(expand.grid(degree,gamma,coef0,cost),MARGIN = 1,function(x){
  svm(binary.quality~.,data=train,kernel="polynomial",degree=x[1],gamma=x[2],coef0=x[3],cost=x[4])
})
models<-c(models,lapply(cost,function(x){
  svm(binary.quality~.,data=train,kernel="linear",cost=x[1])
}))
models<-c(models,apply(expand.grid(gamma,cost),MARGIN = 1,function(x){
  svm(binary.quality~.,data=train,kernel="radial",gamma=x[1],cost=x[2])
}))
models<-c(models,apply(expand.grid(gamma,coef0,cost),MARGIN = 1,function(x){
  svm(binary.quality~.,data=train,kernel="sigmoid",gamma=x[1],coef0=x[2],cost=x[3])
}))

#Use the models to predict
df<-as.data.frame(t(sapply(models,function(x){validationError<-mean((predict(object = x,newdata=validation))!=(validation$binary.quality))
trainError<-mean((predict(object = x))!=(train$binary.quality))
list(kernel = x$call$kernel,cost=x$cost,gamma=x$gamma,coef0=x$coef0,degree=x$degree,trainError=trainError,validationError=validationError )
  })))


View(df[order(as.numeric(df$validationError)),])
```	


```{r}
  
require(randomForest)

rf.class <- randomForest(binary.quality ~ ., data = train, ntree = 30)
rf.class.tuned <- randomForest(binary.quality ~ ., data = train, ntree = which.min(rf.class$err.rate[,1]))
pred <- predict(rf.class.tuned, validation[,-12])
rf.mce <- MCE(pred, validation[,12])

# PCA RF CLASS

PCA <- prcomp(train[,-12], center = FALSE, scale. = FALSE)

rf.mce.PCA <- c()

for (i in 1:2){
  if (i == 1){
    PCA.train <- cbind(data.frame(PC1 = PCA$x[,1:i]), binary.quality = train[,12])
  } else {
    PCA.train <- cbind(as.data.frame(PCA$x[,1:i]), binary.quality = train[,12])
  }
  rf.class <- randomForest(binary.quality ~ ., data = PCA.train, ntree = 30)
  rf.class.tuned <- randomForest(binary.quality ~ ., data = PCA.train, ntree =   which.min(rf.class$err.rate[,1]))
  
  if (i == 1){PCA.validation <- as.matrix(validation[,-12]) %*% PCA$rotation[,1:i]
  PCA.validation <- data.frame(PC1 = PCA.validation)
  } else {PCA.validation <- as.data.frame((as.matrix(validation[,-12]) %*%
                                             PCA$rotation)[,1:i])}
  
  pred <- predict(rf.class.tuned, PCA.validation)
  rf.mce.PCA[i] <- MCE(pred, validation[,12])}

```

```{r}

require(ada)

adaboost <- ada(train[,1:11], train[,12], iter = 10)
adaboost.tuned <- ada(train[,1:11], train[,12], iter = 5)
pred <- predict(adaboost.tuned, validation[,-12])
ada.mce <- MCE(pred, validation[,12])

```

```{r}

# ^2 och ^3 ....
for (i in seq(0,1, 0.1)){
  log.reg <- glm(binary.quality ~ ., data = train, family = "binomial")
  pred <- predict.glm(log.reg, newdata = validation, type = "response")
  pred <- pred > i
  mce.log.reg <- MCE(pred, validation$binary.quality)
  print(mce.log.reg)}

PCA <- prcomp(train[,-12], center = FALSE, scale. = FALSE)  

for (i in 1:11){
  
  if (i == 1){PCA.train <- cbind(data.frame(PC1 = PCA$x[,1:i]), binary.quality = train[,12])
  } else {PCA.train <- cbind(as.data.frame(PCA$x[,1:i]), binary.quality = train[,12])}
  
  if (i == 1){PCA.validation <- as.matrix(validation[,-12]) %*% PCA$rotation[,1:i]
  PCA.validation <- data.frame(PC1 = PCA.validation)
  } else {PCA.validation <- as.data.frame((as.matrix(validation[,-12]) %*%
                                             PCA$rotation)[,1:i])}
  
  log.reg <- glm(binary.quality ~ ., data = PCA.train, family = "binomial")
  pred <- predict.glm(log.reg, newdata = PCA.validation, type = "response")
  pred <- pred > 0.5
  print(MCE(pred, validation$binary.quality))}

require(ROCR)

log.reg <- glm(binary.quality ~ ., data = train, family = "binomial")
pred <- predict.glm(log.reg, newdata = validation, type = "response")
pred.rocr <- prediction(pred, validation$binary.quality)

# accuarcy
acc <- performance(pred.rocr, "acc")
plot(acc)

# precision
prec <- performance(pred.rocr, "ppv")
plot(prec)

# recall
rec <- performance(pred.rocr, "tpr")
plot(rec)

tot <- as.vector(acc@y.values)[[1]] + as.vector(prec@y.values)[[1]] + as.vector(rec@y.values)[[1]]
plot(as.vector(acc@x.values)[[1]], tot)

for (i in 1:15){
  tot <- as.vector(acc@y.values)[[1]] + i * as.vector(prec@y.values)[[1]] + as.vector(rec@y.values)[[1]]
  plot(as.vector(acc@x.values)[[1]], tot)
}

```