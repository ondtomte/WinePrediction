---
title: "Project"
author: "Simon Almerstr?m Przybyl & Daniel ?hman"
date: "July 27th 2015"
output: html_document
---

#1. INTRO / EXPLAIN DATA SET AND TASK

For our project, we have chosen to examine a wine data set. The data set contains measures of chemical levels in wine as its input variables, and the quality of wine as its target variable. The measured chemical levels ranged from the pH level of the wine to the alcohol percentage of the wine, in total 11 different chemical levels were measured on a continuous linear scale. The quality of the wine was measured on a discrete scale from 0 to 10, though the actual observed range was only from 3 to 9. It should also be noted that the quality of the wine was graded by letting individuals taste the wine, the quality scale therefore reflects expert opinion.

```{r}
#download.file(url="https://archive.ics.uci.edu/ml/machine-learning-databases/wi#ne-qual#ity/winequality-white.csv",destfile = "winequality-white.csv")
wine<-read.csv("winequality-white.csv",sep=";")
str(wine)
```

Our task is thus to investigate how different chemical levels affect the quality of the wine, and we split this task into two subtasks:

1 - We view the quality of wine as a continuous variable allowed to take any value on the interval [0,10], this view of the problem yields a regression problem.

2 - We define a good wine to be a wine having quality larger than or equal to 6, and a poor wine to be a wine having quality strictly less than 6. This yields a binary classification problem of separating good wine from poor wine.

Regarding the accuracy of the different subtasks:

In the regression subtask we are of course seeking to minimize the MSE as far as possible. Moreover, we definitely want our regression model to give a lower MSE than the MSE we obtain in our baseline model, this model consists of simply guessing the mean quality of the training.
```{r}
hist(wine$quality,col="red",main="White wine quality distribution",xlab="Quality")
```

#Note that the histogram plots the quality of the training observations, we will describe the process of #splitting the data into training, validation and test sets more thorughly later.

```{r}
MSE(mean(train$quality), validation$quality)
```

Since our training data is approximately normally distrubuted around 6 (with of variance of 0.7723022), our baseline model will perform quite well (this model yields a validation MSE of 0.7705384 and a test MSE of 0.7779192). Therefore, in order to justify the existence of our more sophisticated models, we want them to give a much lower MSE.

How low we want to push down the MSE for us to be satisfied with our models is a difficult question and depends on how are models are supposed to be used. We believe a resonable purpose for our models is to act as a tool for beverage stores when they decide which wine they should keep in stock. However, it is still difficult to define how low the MSE should be for us to be content: When one is for example constructing a bridge, one will probably want the prediction of the target value to be precise enough so the probability of the bridge collapsing is negligent. In our case, our regression models can not really be said to have such a clear goal: We can basically only tell if one given model is better than another given model, not if they are good enough in an absolute sense.

Given the discussion above, any quantitative measure of how much we want to reduce our test MSE is completely arbitrary. However, we pretend that the marketing department of the beverage store has demanded that the test MSE of a sufficient model should be 50% lower than the test MSE of our basline model, such a reduction in MSE would let the marketing department advertise our model as being "50% better than chance".

OLD: In the classification subtask, our main interest is to have a high precision rate, intuitively because we seek to minimize the risk of someone intending to buy a good wine but actually getting a poor wine. We believe a 90% precision rate is sufficiently high for this purpose.

NEW: In the classification subtask, we seek to maximize model accuarcy (i.e. minimze the MCE). The discussion that we held regarding our quantitative demands for the regression models applies directly to the classification models as well: It is difficult for us to specify how a classification model has to perform in order to be be considered sufficiently good, therefore we will pretend that we for purposes of marketing want a classification model to perform twice as good as our baseline classification model. In the classification case the baseline consists of guessing that a new observation belong to the same class as the majority of the obervations in the training data.

KANSKE: In the discussion at the end of this report, we will also discuss alternative measures which could be used to compare the performance of classification models.

END NEW


#2. DATA CLEANING

We downloaded the data set from the UCI Machine Learning Repository [link]( https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/). At the repository, data regarding both white and red wine was available in two separate files, however we chose to only use the data for the white wine since we believe that the quality of wine with different colours may be affected differently by varying chemical levels. Moreover, expert opinion varies when judging white wine compared to red wine.

The data set did not contain any unknown values. Initially, we did not search for any outliers but after having constructed a few models, we noticed that our validation MSE was systematically lower than our training MSE. When this was noticed, we tried to resplit our data into training, validation and test sets a few times, however the pecularity of obtaining lower validation MSE than training MSE was present for most of the splits. The pecularity was especially pronounced for the simpler models. This lead us to believe that there were outliers present in our data: We believe that having outliers present in the training data will not affect the overall behavior of simpler models but will push up the training MSE highly for these models (since the prediction for the outliers will be far off).

We searched for outliers (i.e. observations deviating extremely) by examining the Euclidean norm of the attributes of the observations:

```{r}
require(scales)
wine <- cbind(as.data.frame(scale(wine[,-12],center = TRUE)),quality=wine[,12])
wine2 <- wine
euc <- sqrt(wine[,1]^2+wine[,2]^2+wine[,3]^2+wine[,4]^2+wine[,5]^2+wine[,6]^2+wine[,7]^2+wine[,8]^2+wine[,9]^2+wine[,10]^2+wine[,11]^2)
plot(sort(euc),type="l", xlab = "Sorted index", ylab = "Euclidean norm")
sorted<-sort(euc, index.return = TRUE)$ix
wine<-wine[-sorted[c(1:80,(length(sorted)-200):length(sorted))],]
euc <- sqrt(wine[,1]^2+wine[,2]^2+wine[,3]^2+wine[,4]^2+wine[,5]^2+wine[,6]^2+wine[,7]^2+wine[,8]^2+wine[,9]^2+wine[,10]^2+wine[,11]^2)
lines(y=sort(euc),x=51:(nrow(wine)+50), col = alpha("red", 0.5), lw=6)
```

```{r}
baseline.MSE <- MSE(mean(train$quality), validation$quality)
```

The black line shows the Euclidean norm of all of the observations in the wine data set, ranging from the one with the lowest norm to the one with the highest. Since there are clearly a few observations deviating extremely in terms of their Euclidean norm, we chose to remove these observations and only keep the line segment marked in red. The following histogram shows the quality distribution of the removed observations:

```{r}
hist(wine2$quality[sorted[c(1:80, (length(sorted)-200):length(sorted))]], col = "red", main = "Removed data", xlab = "Quality")
```

The histogram shows that the removed observations did not come from any particular quality class (they are distributed similarly to the training data set).

RUBRIK

The wine data set was split into training, validation and test sets by doing a random 64-16-20 split while preserving relative ratios of different labels of the target variable (note that this split is done by first doing a 80-20 split into a combined training-validation set and test set, followed by a 80-20 split of the combined training-validation set). For the binary classification we created a new target variable which is either true or false depending on if the wine is good or poor. Note that the same data split was used for both the regression problem and the classification problem. Finally, we scaled and centered all of the attributes except for the target variable.

```{r}
GOOD_WINE=5
library(caTools)
set.seed(24)
s <- sample.split(Y = wine$quality,SplitRatio = 0.8)
test <- wine[s==FALSE,]
temp <-wine[s==TRUE,]
s <- sample.split(Y = temp$quality,SplitRatio = 0.8)
validation <- temp[s==FALSE,]
train <- temp[s==TRUE,]
```

#3 & 4. FEATURE IMPORTANCE SCORES / PCA

We used PCA to find the successive orthogonal directions of maximum variance:

```{r}
PCA <- prcomp(train[,-12], center = FALSE, scale. = FALSE)
PCA$sdev
```

Intuitively PCA defines a set of new base vectors of our data (corresponding to a rotation) and also gives the standard deviations of our data in these directions, the idea is to keep the directions of high variance since these are assumed to be the most informative (at least given a linear relation between the attrbiutes and the target variable).

Since the standard deviations decrease quite slowly without any sharp decline, except for the step from the 10th to the 11th principle component, there is not any natural cut-off point for how many principle components we should choose. We therefore choose to express our data using the first i principle components, for every i from 1 to 11, and create a new data frame for each such transformation.

We also create new data frames where we have projected our original data onto 2nd and 3rd order polynomials, note that these polynomials contain 11 variables and thus have a lot of cross terms. We call these these data frames poly2 and poly3 respectively, while our original data frame is called normal. We do not project onto polynomials of higher order because of computational limitations, and we do not believe that such polynomials would yield better models since they would most likely overfit to the training data. When we create models, we will simultaneously create a given model (for example linear regression) for all of these data frames.

## PCA

```{r, echo=FALSE}
#Create data set with diffrent attribute
trainPoly<-lapply(2:3,function(x){
  foo<- with(train,polym(fixed.acidity,volatile.acidity,citric.acid,residual.sugar,chlorides,free.sulfur.dioxide,total.sulfur.dioxide,density,pH,sulphates,alcohol,degree = x,raw=TRUE))
foo<-as.data.frame(foo)
foo$quality=train$quality
list(foo)
})

validationPoly<-lapply(2:3,function(x){
  foo<- with(validation,polym(fixed.acidity,volatile.acidity,citric.acid,residual.sugar,chlorides,free.sulfur.dioxide,total.sulfur.dioxide,density,pH,sulphates,alcohol,degree = x,raw=TRUE))
  foo<-as.data.frame(foo)
  list(foo)
})


PCA <- prcomp(train[,-12], center = FALSE, scale. = FALSE)
PCA.validation<-lapply(1:11, function(i){
    if (i == 1){
    foo <- as.matrix(validation[,-12]) %*% PCA$rotation[,1:i]
    list(data.frame(PC1 = foo))
  } else {
     list(as.data.frame((as.matrix(validation[,-12]) %*%
                                         PCA$rotation)[,1:i]))
  }
})

PCA.train<-as.data.frame(cbind(PCA$x,quality=train$quality))

dataSets<- list(normal=list(train=train,validation=validation),
                poly2=list(train=as.data.frame(trainPoly[[1]]),validation=as.data.frame(validationPoly[[1]])),
                poly3=list(train=as.data.frame(trainPoly[[2]]),validation=as.data.frame(validationPoly[[2]])),
                pca10=list(train=PCA.train[,c(1:10,12)],validation=as.data.frame(PCA.validation[[10]])),
                pca9=list(train=PCA.train[,c(1:9,12)],validation=as.data.frame(PCA.validation[[9]])),
                pca8=list(train=PCA.train[,c(1:8,12)],validation=as.data.frame(PCA.validation[[8]])),
                pca7=list(train=PCA.train[,c(1:7,12)],validation=as.data.frame(PCA.validation[[7]])),
                pca6=list(train=PCA.train[,c(1:6,12)],validation=as.data.frame(PCA.validation[[6]])),
                pca5=list(train=PCA.train[,c(1:5,12)],validation=as.data.frame(PCA.validation[[5]])),
                pca4=list(train=PCA.train[,c(1:4,12)],validation=as.data.frame(PCA.validation[[4]])),
                pca3=list(train=PCA.train[,c(1:3,12)],validation=as.data.frame(PCA.validation[[3]])),
                pca2=list(train=PCA.train[,c(1:2,12)],validation=as.data.frame(PCA.validation[[2]])),
                pca1=list(train=PCA.train[,c(1,12)],validation=as.data.frame(PCA.validation[[1]])))
                
dataSets$normal$validation$quality=NULL
```

We examine the linear dependency between the attributes and return those who are most linearly dependent (those with largest absolute value of correlation):

```{r}
#cor
cor_wine<-cor(train)
cor_wine[lower.tri(cor_wine,diag=TRUE)] <-NA
df_wine <- as.data.frame(as.table(cor_wine))
df_wine <- df_wine[order(-abs(df_wine$Freq)),]
#Top 10 highest correlated attributes
colnames(df_wine)<- c("Var1","Var2","Cor")
df_wine[1:10,]
```

If we were to remove some features, it would be reasonable to remove for example residual.sugar and total.sulfur.dioxide since density already provides information about these attributes.

KOMMENTAR.

We also examine the mutual information criterion:

```{r}
#Shared information
require(plyr)
require(reshape)
require(entropy)
my_fun<-function(x){freq <- count(train,c("quality",x))
colnames(freq) <- c("Var1","Var2","Freq")
freq2d<-cast(freq, Var1 ~ Var2,fill=0,value ="Freq")
mi.plugin(freqs2d = freq2d)}

cn<- colnames(wine[,-12])
mi <- sapply(cn,my_fun)
sort(mi,decreasing = TRUE)
```

The fact that density has the highest mutual information criterion intuitively matches our results about the linear dependency of the attributes: By using the linear dependencies, knowing the density of wine provides us with information about its residual.sugar, alcohol and total.sulfur.dioxide values. Knowing four attributes, we most likely have a higher probability of knowing the quality of the wine than if we only know one attribute.

#5. MODELS

##5.1 REGRESSION

```{r, echo=FALSE}
MSE <- function(x,y){
  mean((x - y)^2, na.rm = TRUE)}
```

We begin by simply regressing quality on each of the individual attributes. The highest mutual information was observed between quality and density, we therefore naively except this model to perform best (given that the relation between density and quality is actually linear, plotting the data shows a slight such tendency):

```{r}
lm.models <- list()
for (i in 1:11) {
  model<-lm(as.formula(paste("quality~", names(train)[i])),train)
  pred<-predict(model,validation)
  lm.models[[names(train)[i]]] <- list(validationMSE=MSE(pred,validation[12]),
  trainMSE=mean(model$residuals^2))}
```

```{r}
mse.lm.models<-sapply(lm.models,function(x){
  x$validationMSE
})

barplot(mse.lm.models,main="Single Variable LM Models", ylab="Validation MSE",las=2, ylim = c(0,.9))
abline(h = baseline.MSE, lty = 2)
text(12,MSE(mean(train$quality), validation$quality)+0.05,"baseline = 0.7759")

# MAGISK SIFFRA
```

```{r}
proc.base <- function(x){100 * (baseline.MSE - x)/baseline.MSE}
```

The models performed badly, the only model with descent performance was the alcohol model (note that alcohol occured frequently in the correlation scores) with a validation MSE of `mse.lm.models[11]` corresponding to a `proc.base(mse.lm.models[11])`% procentual improvement from baseline.

We continue with more advanced linear models:

```{r,cache=TRUE}
library(boot)
linear.models <- lapply(dataSets, function(x){
  model<-glm(quality~.,data=x$train)
  pred<-predict(model,as.data.frame(x$validation))
  cv<-cv.glm(as.data.frame(x$train), model, K = 5)
  list(validationMSE=MSE(pred,validation[12]),
  trainMSE=mean(model$residuals^2),
  trainCV=cv$delta[1],mceValidation=mean((pred<GOOD_WINE)!=(validation$quality<GOOD_WINE)),model=model)
})
```
```{r, echo=FALSE}
mse.linearV<-sapply(linear.models,function(x){
  x$validationMSE
})
mse.linearT<-sapply(linear.models,function(x){
  x$trainMSE
})
mse.linearCV<-sapply(linear.models,function(x){
  x$trainCV
})

barplot(t(cbind(mse.linearV,mse.linearT,mse.linearCV)),main="Linear regression", ylab="MSE",las=2,beside = TRUE,legend=c("Validation","Train","Train CV"),args.legend = list(x="bottomright"))
```

Poly2 performs best with a validation MSE of `mse.linearV[2]` (`proc.base(mse.linearV[2])`% improvement over baseline).

The difference in validation MSE between the best models is quite small. Polynomial regression of degree 3 seems to overfit (since the difference between its training and validation MSE is high compared to the other models), yet still performs second best. It can also be noted that the CV errors (raw and adjusted) lie close to the acutal validation errors, as expected. Finally, the more PCA components we use the better the model gets, suggesting that all variables are relevant (it can also be noted that using all PCA components gives the same result as performing linear regression on the original data, as expected).

We also construct a set of lasso models, these models give a penalty on the absolute value of the linear coefficients, a parameter lambda decides the weight of the penalty. These models effectively corresponds to successively removing our attributes (as $\lambda$ grows larger) and might also thus give us information regarding if some of our attributes are redundant:

```{r, cache=TRUE}
require(lars)

lasso.models <- lapply( dataSets, function(x){
  m<-as.matrix(x$train[-ncol(x$train)])
  models<-lars(x = m, y = as.numeric(x$train$quality), type = "lasso")
  
  cv<-cv.lars(x = m, y = as.numeric(x$train$quality), type = "lasso", plot.it = FALSE, K = 5)
  pred<-predict(models, as.matrix(x$validation))
  mse<-sapply(1:ncol(pred$fit),function(x){MSE(pred$fit[,x],validation$quality)})
  bestMse<-list(min=min(mse),bestLassoModel=which.min(mse))
  list(validationMSE=mse,CV=cv, model=models,bestMse=bestMse,bestMceValidation=mean((pred$fit[,bestMse$bestLassoModel]>GOOD_WINE)!=(validation$quality>GOOD_WINE)))
})
```

```{r, echo = FALSE}
mse.lassoV<-sapply(lasso.models,function(x){
  x$bestMse$min
})

barplot(mse.lassoV,main="Best Lasso Model", ylab="Validation MSE",las=2)
```

The lasso models corresponding to polynomial regression of degree 3 gave the lowest validation MSE (`mse.lassoV[3]`, a `proc.base(mse.lassoV[3])`%) improvement over the baseline). This can actually be said to match the results from our ordinary linear models: Lasso regression on the poly3 data approximately corresponds to successively removing terms from the fitted 3rd order polynomial. Since polynomial regression of degree 2 gave the best linear model, and polynomial regression of degree 3 gave the second best, it seems reasonable to find an even better polynomial fit if we let the degree of the polynomial "lie between" 2 and 3.

```{r}
lambda<-rev(c(lasso.models$poly3$model$lambda,0))
plot(x=lambda,y=rev(lasso.models$poly3$validationMSE),type="l",xlab=expression(lambda),ylab="Validation MSE", main="Lasso Poly3")
```

Moreover, we also construct ridge models which penalize the square of the linear coefficients:

```{r,eval=TRUE,cache=TRUE}
predict.ridge<-function(model,x,i){
  scale(x,center = model$xm,scale = model$scales)%*%model$coef[i,] +model$ym
}

library(MASS)
ridge.models<-lapply(dataSets[-13],function(x){
  models<-lm.ridge(formula = quality~.,data=x$train,lambda = seq(0,1500,by=10))
  predictionsValidation <- scale(x$validation,center = models$xm,scale = models$scales)%*%models$coef +models$ym
  validationMSE <- apply(predictionsValidation,MARGIN=2,MSE,y=validation$quality)
  
  predictionsTrain<-  scale(x$train[-ncol(x$train)],center = models$xm,scale = models$scales)%*%models$coef +models$ym
  trainMSE<-apply(predictionsTrain,MARGIN=2,MSE,y=train$quality)
  
  bestRidgeModel<-list(bestMse=min(validationMSE),bestModel=which.min(validationMSE))
  mceValidation<-mean( (predictionsValidation[,bestRidgeModel$bestModel]>GOOD_WINE)!=(validation$quality>GOOD_WINE))
    
  list(trainMSE=trainMSE, validationMSE= validationMSE, model=models,bestRidgeModel=bestRidgeModel,mceValidation=mceValidation)
})
```

```{r}
mse.ridgeV<-sapply(ridge.models,function(x){
  x$bestRidgeModel$bestMse
})

mse.ridgeT<-sapply(ridge.models,function(x){
  x$trainMSE[x$bestRidgeModel$bestModel]
})
barplot(t(cbind(mse.ridgeV,mse.ridgeT)),main="Best Ridge regression", ylab="MSE",las=2,beside = TRUE,legend=c("Validation","Train"),args.legend = list(x="bottomright"))
```

The best ridge model was poly3 with $\lambda$ equal to `seq(0,1500,by=10)[which.min(ridge.models$poly3$validationMSE)]`, this model gave a validation MSE of `mse.ridgeV[3]` (`proc.base(mse.ridgeV[3])`% better than baseline).

TODO: Fundera på rimlighet gällande lambda i lasso vs ridge.

The following plots show the how lambda affects the training and validation MSE for four selected data sets:

```{r, echo=FALSE}
par(mfrow=c(2,2))
plot(y=ridge.models$poly3$trainMSE,x=seq(0,1500,by=10),type="l",ylab="MSE",xlab=expression(lambda),ylim = c(0.35,0.58),col="red",main="Poly3")
lines(y=ridge.models$poly3$validationMSE,x=seq(0,1500,by=10),type="l",col="blue")

plot(y=ridge.models$poly2$trainMSE,x=seq(0,1500,by=10),type="l",ylab="MSE",xlab=expression(lambda),ylim = c(min(ridge.models$poly2$trainMSE),max(ridge.models$poly2$validationMSE)),col="red",main="Poly2")
lines(y=ridge.models$poly2$validationMSE,x=seq(0,1500,by=10),type="l",col="blue")

plot(y=ridge.models$normal$trainMSE,x=seq(0,1500,by=10),type="l",ylab="MSE",xlab=expression(lambda),ylim = c(min(ridge.models$normal$trainMSE),max(ridge.models$normal$validationMSE)),col="red",main="Normal")
lines(y=ridge.models$normal$validationMSE,x=seq(0,1500,by=10),type="l",col="blue")

plot(y=ridge.models$pca4$trainMSE,x=seq(0,1500,by=10),type="l",ylab="MSE",xlab=expression(lambda),ylim = c(min(ridge.models$pca4$trainMSE),max(ridge.models$pca4$validationMSE)),col="red",main="PCA4")
lines(y=ridge.models$pca4$validationMSE,x=seq(0,1500,by=10),type="l",col="blue")
```

```{r,eval=FALSE, echo=FALSE}
#Learninng rate
#TODO Make a function
#plot.learning.rate<-function(model,data,min,max){
#  set.seed(23)
#  
#  
#}
set.seed(23)
n<-seq(500,4000, by=100)
mse.train<- vector()
mse.validation<-vector()
for(i in n){
  temp.mse.train<-0
  temp.mse.validation<-0
  for(j in 1:100){
    k<- sample(x = 1:nrow(wine),size =i,replace = FALSE)
    subset <- wine[k,]
    s<- sample.split(subset, SplitRatio = 2/3)
    trainSubset<- subset[s==TRUE,]
    validationSubset<-subset[s==FALSE,]
    mod<-lm(quality~.^3,data=trainSubset)
    temp.mse.train<-mean((predict(mod)-trainSubset$quality)^2)+temp.mse.train
    temp.mse.validation<-mean((predict(mod,newdata=validationSubset)-validationSubset$quality)^2)+     temp.mse.validation
    
  }
  mse.train = c(mse.train,temp.mse.train/100)
  mse.validation=c(mse.validation,temp.mse.validation/100)

}

plot(x=n,y=mse.validation,type="l",col="blue",ylim=c(min(mse.train),min(c(2,max(mse.validation)))))
lines(x=n,y=mse.train,type="l", col="red")
```

We know perform a thin plate spline on the PCA1 to PCA10 data sets, we ignore the other data sets for the construction of this model since they give error messages regarding colinearity. The thin plate spline creates a smooth surface approximating the data points, technically this is done by projecting the data points to a higher dimensional space and then applying ridge regression. The lambda parameter of the ridge regression defines the smoothness of this surface and is chosen by the following function through cross-validation.

TODO: Varför använde inte övningar?

```{r,cache=TRUE,eval=TRUE,warning=FALSE}
require(fields)
# avoiding colinearity
remove <- c(1,2,3,4)
tps.models <- lapply(dataSets[-remove], function(x){
  model <- Tps(as.matrix(x$train[,-ncol(x$train)]), x$train[,ncol(x$train)], scale.type = "unscaled")
  pred <- predict(model, as.matrix(x$validation))
  list(validationMSE=MSE(pred,validation[,12]),trainMSE=mean(model$residuals^2),mceValidation=mean((pred>GOOD_WINE)!=(validation$quality>GOOD_WINE)))
})
```

```{r,cache=TRUE,eval=FALSE}
tps.valid <-rev(sapply(tps.models, function(x){x$validationMSE}))
tps.train <-rev(sapply(tps.models, function(x){x$trainMSE}))
barplot(rbind(tps.valid,tps.train), ylab = "Validation MSE", las = 2, ylim = c(0, 1), beside = TRUE,legend=c("Validation","Train"),args.legend = list(x="bottomleft"))
```

PCA7 gave the lowest thin plate spline validation MSE, `tps.valid[7]` (a `proc.base(tps.valid[7])` improvement over baseline). The PCA5, PCA7 and PCA9 models all have a very low training MSE, in the case of PCA9 this seems to cause overfitting while PCA7 performs suprisingly well, we do not have any intuitive explanation of this behavior.

We now construct a set of local regressions models, these models perform regression around every data point in the training data set together with its n-closest neighbors:

```{r, echo=FALSE}
cust.col <- append(colors(distinct = TRUE)[5],colors(distinct = TRUE)[5 + seq(2,length(dataSets))*30])

generic.plot <- function(x.cord, x.lim, y.lim, x.name, x){
    plot(x.cord, x[[1]], col = cust.col[1], lwd = 3, type = "l", xlim = x.lim, ylim =
    y.lim, xlab = x.name, ylab = "Validation MCE")
    for (i in 2:length(x)){
      lines(x.cord, x[[i]], col = cust.col[i], lwd = 3)}
    legend("topright", names(x), col=cust.col, lty=1, cex = 0.5)}
# COLOR PROBLEM, rainbow()...?
```

```{r,eval=TRUE,cache=TRUE,warning=FALSE}
distMatrix=function(N,M){
  eN<-matrix(1,nrow=nrow(N),ncol=ncol(N))
  eM<-matrix(1,nrow=nrow(M),ncol=ncol(N))
  sqrt(eM%*%t(N^2)-2*M%*%t(N)+M^2%*%t(eN))
}

temp.dataSets <- dataSets
temp.dataSets$pca1 <- NULL

neighbors <- seq(30,600, 80)

loc.reg.mse <- lapply(temp.dataSets, function(x){
# PCA1
  matrix <- as.matrix(as.data.frame(x$train))
  distance<-distMatrix(matrix[,-ncol(matrix)],as.matrix(as.data.frame(x$validation)))
  pred <- c()
  validationMSE <- c()
  for (r in neighbors){
    for(i in 1:nrow(validation)){
      n<-sort(distance[i,],index.return = TRUE)$ix[1:r]
      mod<-lm(quality~.,data=as.data.frame(x$train)[n,])
      pred[i]<-predict(mod,as.data.frame(x$validation)[i,])}
      validationMSE <- c(validationMSE, MSE(pred,validation[12]))}
  validationMSE
})

# TODO: Tränings-MSE
loc.reg.Vmse <- sapply(loc.reg.mse, function(x){min(x)})
```

```{r,eval=TRUE,cache=TRUE,warning=FALSE}
generic.plot(neighbors, c(30, 680), c(0.45,0.75), "Number of neighbors", loc.reg.mse)
abline(h = mse.linearV[1], lty = 2)
```

We constructed our local regression models with 30 to 590 neighbors, stepping by 80. The best model was the normal data set with 190 neighbors, giving a validation MSE of `loc.reg.mse$normal[3]` corresponding to a `proc.base(loc.reg.mse$normal[3])`% improvement of baseline.

Since local regression with as many neighbors as there are observations by definition is ordinary linear regression, and since the validation MSE for the linear regression of the normal data set was `mse.linearV[1]`, it seems as if the validation MSE of local regression for the normal data set converges from below to the validation MSE of linear regression as the number of neighbors grow.

We continue with random forest models. A random forest model randomly chooses a number of features and thereafter uses bootstrap samples of the training data to construct a tree. By repeating this procedure, several trees are constructed and the prediction on a new data set is then done by averaging the prediction over the different trees (also note that each individual tree predicts by averaging the value of the training cases in the leaf which the new case belongs to).

```{r,eval=TRUE,cache=TRUE}
require(randomForest)
randomForest.models <- lapply(dataSets, function(x){
  model<-randomForest(quality ~ ., data=x$train, ntree = 500)
  tuned.model <- randomForest(quality ~ ., data=x$train, ntree =
                                which.min(model$mse))
  pred<-predict(tuned.model,as.data.frame(x$validation))
  list(validationMSE=MSE(pred,validation$quality),trainMSE=tuned.model$mse[length(tuned.model$mse)],tree=which.min(model$mse))
})
```

```{r}
mse.forestV<- sapply(randomForest.models,function(x){
  x$validationMSE
})

mse.forestT<- sapply(randomForest.models,function(x){
  x$trainMSE
})
barplot(t(cbind(mse.forestV,mse.forestT)),main="Random Forest regression", ylab="MSE",las=2,beside = TRUE,legend=c("Validation","Train"),args.legend = list(x="bottomright"))
```

The normal data set with a validation MSE of `randomForest.models$normal$validationMSE` (`proc.base(randomForest.models$normal$validationMSE)`% lower than baseline) performs best, however the poly2 and poly3 data sets only performed slightly worse. Interestingly, the validation MSE for normal is lower than its training MSE, suggesting that overfitting does not occur.

We continue by plotting how the validation and training MSE for the normal data set is affected by the number of trees (we have not tuned the models from which we have obtained the following results). The blue line represents the training MSE and the red represents validation:

```{r,eval=TRUE,cache=TRUE}
x <- dataSets$normal
ntrees <- seq(10, 1000, 50)

fixed.rf <- sapply(ntrees, function(y){
  model<-randomForest(quality ~ ., data=x$train, ntree = y)
  pred<-predict(model,as.data.frame(x$validation))
  list(validationMSE=MSE(pred,validation$quality),trainMSE=model$mse[length(model$mse)])
})
```

```{r,eval=TRUE,cache=TRUE}
valid <- c()
for (i in 1:length(fixed.rf[1,])){
  valid[i] <- fixed.rf[1,][[i]]
}

tr <- c()
for (i in 1:length(fixed.rf[2,])){
  tr[i] <- fixed.rf[2,][[i]]
  }

plot(seq(10, 1000, 50), valid, type = "l", col = "red", ylab = "Validation MSE", xlab = "Number of trees")
lines(seq(10, 1000, 50), tr, col = "blue")
```

```{r,eval=TRUE,cache=TRUE}
best.V <- c("baseline" = baseline.MSE, "1 var. LM" = min(mse.lm.models), "LM" = min(mse.linearV), "lasso" = min(mse.lassoV), "ridge" = min(mse.ridgeV), "TPS" = min(tps.valid), "local reg." = min(loc.reg.Vmse), "RF" = min(mse.forestV))

barplot(best.V, las = 2, col = c(cust.col[1],cust.col[1],cust.col[2],cust.col[3],cust.col[3],cust.col[7],cust.col[1],cust.col[1]), ylim = c(0,0.8), main = "Validation MSE, Best Data Sets")
legend("topright", names(dataSets)[c(1,2,3,7)], col= cust.col[c(1,2,3,7)], lty=1, lwd = 5, cex = 0.65, horiz = TRUE)
```

```{r,eval=TRUE,cache=TRUE}
baseline.MSE.new <- rep(NA, 13)
baseline.MSE.new[1] <- baseline.MSE

tps.valid.new <- rep(NA, 4)
tps.valid.new <- c(tps.valid.new, rev(tps.valid))

loc.reg.Vmse.new <- c(loc.reg.Vmse, NA)

mse.ridgeV.new <- c(mse.ridgeV, NA)


bar.matrix <- matrix(c(baseline.MSE.new, mse.linearV, mse.lassoV, mse.ridgeV.new, tps.valid.new, loc.reg.Vmse.new, mse.forestV, rep(NA, 13), rep(NA, 13)), ncol = 9)


barplot(bar.matrix, beside = TRUE, ylim = c(0,0.8), names = c("Baseline","LM","Lasso","Ridge","TPS","Local reg.","RF", "", ""), las = 2, col = cust.col)
legend("topright", names(dataSets), col= cust.col, lty=1, lwd = 5, cex = 0.65)
```

#5.x Regression models for classification
In a way to establish a new better baseline for the classification part. We decided to try our regression models for classification.
```{r,eval=FALSE}
#Linear
linear.mce<-sapply(linear.models,function(x){
  x$mceValidation
})
barplot(linear.mce,las=2,main="Linear models",ylim = c(0,0.45))

#Lasso
lasso.mce<-sapply(lasso.models,function(x){
  x$bestMceValidation
})
barplot(lasso.mce,las=2,main="Lasso models",ylim = c(0,0.45))

#Ridge
ridge.mce<- sapply(ridge.models,function(x){
  x$mceValidation
})
barplot(ridge.mce,las=2,main="Ridge models",ylim = c(0,0.45))


```

##5.1 CLASSIFICATION

	
```{r, echo = FALSE}
MCE <- function(x, y)
  {mean(x != y)}
```


	
```{r, echo = FALSE}
MIN_PRECISION=0.75
validation$binary.quality <- validation$quality >= 6
validation$quality=NULL
validation$binary.quality<-as.factor(validation$binary.quality)

for(i in 1:length(dataSets)){
  dataSets[[i]]$train$binary.quality<-(dataSets[[i]]$train$quality>GOOD_WINE)
  dataSets[[i]]$train$binary.quality<-as.factor(dataSets[[i]]$train$binary.quality)
  dataSets[[i]]$train$quality<-NULL
}

baseline.misclass <- MCE(TRUE, validation$binary.quality)
baseline.misclass
```

Our first classification model will be a KNN model where we classify a new observation according to which class its K-nearest euclidean neighbors belong to (letting the majority decide the outcome and if a tie occurs, letting the K-1 nearest neighbors decide the outcome).



```{r,eval=TRUE, warning = FALSE}
require(FNN)
neighbors <- c(3,5,7,10,20,40,60,100,150,200)
knn.models<- lapply(dataSets,function(x){
  mce<-sapply(neighbors, function(k){
    pred<-knn(train = x$train[-ncol(x$train)],test = x$validation,cl = x$train$binary.quality,k=k)
    MCE(pred,validation$binary.quality)
  })
#  list(mce=mce,best=list(bestMce=min(mce)),k=neighbors[which.min(mce)])
})

#mce<-sapply(knn.models, function(x){
#  x$best$bestMce
#})

generic.plot(neighbors, c(0,240), c(.17,.33), "Number of neighbors", knn.models)
```

The best KNN model turns out to be the dataset poly3 with K = 5, giving a validation MCE of 0.1935047.

We continue by constructing LDA and QDA models:

The LDA model calculates the mean of the attributes for a given class, and then constructs a Gaussian kernel with such a mean and variance 1. This gives rise to two probability estimates when a new observation is to be classified, the observation is then classified according to which of these estimates are the largest. Note that this method is equivalent to classifying a new observation according to which class mean it is closest to.

The QDA model works similarly but in this case the variance is not set to 1 but instead to the variance of the attributes of the each individual class. 

```{r,eval=FALSE}
require(MASS)

# remove poly2 and poly3?

lda.qda.models <- lapply(dataSets, function(x){
  wine.lda.cv <- lda(binary.quality ~ ., data = x$train, CV = TRUE)
  wine.qda.cv <- qda(binary.quality ~ ., data = x$train, CV = TRUE)
  wine.lda <- lda(binary.quality ~ ., data = x$train)
  wine.qda <- qda(binary.quality ~ ., data = x$train)

  list(CV.MCE.lda=MCE(wine.lda.cv$class, x$train[,ncol(x$train)]),CV.MCE.qda=MCE(wine.qda.cv$class,
  x$train[,ncol(x$train)]),qda.validation.MCE=MCE(predict(wine.qda, x$validation)$class,
  validation[,12]),lda.validation.MCE=MCE(predict(wine.lda, x$validation)$class, 
  validation[,12]))
})

# barplot
# $posterior + ROCR...
```

RESULTS

The following support vector machine (SVM) models find the hyperplane which best separates the classes given the specified hyperparameters. Note that we only construct this model with the normal dataset due to the large number of hyperparameters.

```{r,eval=TRUE}
require(e1071)

#Hyperparameters
degree<-c(2,3)
gamma<-c(0.10,0.30)
coef0<-c(1,3)
cost<-c(1,3)

#Models for diffrent kernels
models<-apply(expand.grid(degree,gamma,coef0,cost),MARGIN = 1,function(x){
  svm(binary.quality~.,data=dataSets$normal$train,kernel="polynomial",degree=x[1],gamma=x[2],coef0=x[3],cost=x[4])
})
models<-c(models,lapply(cost,function(x){
  svm(binary.quality~.,data=dataSets$normal$train,kernel="linear",cost=x[1])
}))
models<-c(models,apply(expand.grid(gamma,cost),MARGIN = 1,function(x){
  svm(binary.quality~.,data=dataSets$normal$train,kernel="radial",gamma=x[1],cost=x[2])
}))
models<-c(models,apply(expand.grid(gamma,coef0,cost),MARGIN = 1,function(x){
  svm(binary.quality~.,data=dataSets$normal$train,kernel="sigmoid",gamma=x[1],coef0=x[2],cost=x[3])
}))

#Use the models to predict
df<-as.data.frame(t(sapply(models,function(x){validationError<-mean((predict(object = x,newdata=dataSets$normal$validation))!=(validation$binary.quality))
trainError<-mean((predict(object = x))!=(dataSets$normal$train$binary.quality))
list(kernel = x$call$kernel,cost=x$cost,gamma=x$gamma,coef0=x$coef0,degree=x$degree,trainError=trainError,validationError=validationError )
  })))


df[order(as.numeric(df$validationError)),]

# barplot 
# tabell istället? Titta närmare på radial med fler hyperparametrar (coef0 och degree ändras inte?):

#Hyperparameters
degree<-c(3)
gamma<-c(0.01,0.10,0.30,0.50,1)
coef0<-c(1)
cost<-c(1,3,5,10)

models<-c(apply(expand.grid(gamma,cost),MARGIN = 1,function(x){
  svm(binary.quality~.,data=dataSets$normal$train,kernel="radial",gamma=x[1],cost=x[2])
}))

#Use the models to predict
df<-as.data.frame(t(sapply(models,function(x){validationError<-mean((predict(object = x,newdata=dataSets$normal$validation))!=(validation$binary.quality))
trainError<-mean((predict(object = x))!=(dataSets$normal$train$binary.quality))
list(kernel = x$call$kernel,cost=x$cost,gamma=x$gamma,coef0=x$coef0,degree=x$degree,trainError=trainError,validationError=validationError )
  })))


df[order(as.numeric(df$validationError)),]

```	

Best SVM model:

Kernel: radial
Cost: 3
Gamma: 0.5
Coef0: 0
Degree: 3
Train MCE: 0.0192958700067705
Valid MCE: 0.166441136671177

We construct a set of random forest models, DESCRIPTION...

```{r,eval=TRUE, cache=TRUE}
set.seed(56)
require(randomForest)
trees <- c("5" = 5, "10" = 10, "20" = 20, "50" = 50, "100" = 100, "200" = 200)

randomForest.c.models <- lapply(dataSets, function(x){
  lapply(trees, function(y){
    model<-randomForest(binary.quality ~ ., x$train, ntree = y)
#   tuned.model <- randomForest(binary.quality ~ ., x$train, ntree =
#                                  which.min(model$err.rate[,1]))
    pred<-predict(model,as.data.frame(x$validation))
    MCE(pred,validation$binary.quality)
  })
})
#plot?

# OOB gör det vi vill, eller är i träning..?

generic.plot(trees, c(0,240), c(.12,.32), "Number of neighbors", randomForest.c.models)

# gör poly3 med många och tuned?
```

```{r,eval=FALSE}
set.seed(56)
require(ada)

dataSets.foo <- dataSets
dataSets.foo$pca1 <- NULL

iterations <- c("1" = 1, "5" = 5, "10" = 10, "15" = 15, "20" = 20)

ada.models <- lapply(dataSets.foo, function(x){
  lapply(iterations, function(y){
    model<-ada(x$train[,1:ncol(x$train)-1], x$train[,ncol(x$train)], iter = y)
    # tune?
    pred<-predict(model,x$validation)
    MCE(pred,validation$binary.quality)
  })
})

generic.plot(iterations, c(1,23), c(.18,.33), "Number of iterations", ada.models)

# gör poly3 med många och tuned?
```

```{r,eval=FALSE}
require(ROCR)

# returnerna model
logistic.reg <- lapply(dataSets, function(x){
model <- glm(binary.quality ~ ., data = x$train, family = "binomial")
pred <- predict.glm(model, newdata = x$validation, type = "response")
pred.rocr <- prediction(pred, validation$binary.quality)
})

x <- logistic.reg
plot(performance(x[[1]], "acc")@x.values[[1]], 1 - performance(x[[1]], "acc")@y.values[[1]], col =
cust.col[1], xlim = c(0,1.2), ylim = c(.20,.7), lwd = 3, ylab = "Validation MCE", xlab = "Cutoff", type = "l")
for (i in 2:length(x)){
  lines(performance(x[[i]], "acc")@x.values[[1]], 1 - performance(x[[i]], "acc")@y.values[[1]], col =
  cust.col[i], lwd = 3)}
legend("topright", names(x), col=cust.col, lty=1, cex = 0.5)
```
##Results and disc
best model best (lowest mce) feature set.


diskussion om att de nödvändigtvis inte har fått bäst modell efter att de varierat...!


